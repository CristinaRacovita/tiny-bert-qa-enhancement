{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68e8ca0b-2e9c-4884-8d26-a1cdf557a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6533324-85cf-4079-a104-d20a981f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_file_path) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58fc9b90-51ea-463b-87d1-128d5ad44cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(output_file_path, data):\n",
    "    # Open a new JSON file for writing\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        for data_line in data:\n",
    "            output_file.write(json.dumps(data_line) + \"\\n\")\n",
    "\n",
    "    print(f\"Data with 'Test' property added has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "id": "ae87b0ef-e953-4e00-ac30-890583a094dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"./squad_data_validation_pos_ner.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "id": "cd7cba63-71a8-43cf-a764-1e9e2bf4eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_start_position(text, pos_context, answer, context):\n",
    "    # print(text)\n",
    "    sentence = Sentence(text)\n",
    "    tokens = []\n",
    "    spaces_no = context[:answer].count(\" \")\n",
    "    # pos_context = pos_context.replace('\\\\','')\n",
    "\n",
    "    for word in sentence:\n",
    "        tokens.append(word.text)\n",
    "    # print(tokens)\n",
    "\n",
    "    start_position = 0\n",
    "    token_number = 0\n",
    "    match = False\n",
    "    no_match = -1\n",
    "    unicode = False\n",
    "    for start_position in range(len(pos_context) - len(tokens) + 1):\n",
    "        k_words_context_ = [bytes(list(line.values())[0][0], \"utf-8\").decode(\"unicode_escape\") for line in\n",
    "                      pos_context[start_position:start_position + len(tokens)]]\n",
    "        k_words_context = [list(line.values())[0][0].replace('\\\\','') for line in\n",
    "                      pos_context[start_position:start_position + len(tokens)]]\n",
    "        next_word = [list(line.values())[0][0] for line in\n",
    "                      pos_context[start_position + len(tokens):start_position + len(tokens) + 1]]\n",
    "        # print(k_words_context)\n",
    "\n",
    "        if context.count(text) == 1 and k_words_context == tokens:\n",
    "            match = True\n",
    "            break\n",
    "\n",
    "        if k_words_context == tokens and (token_number + spaces_no == answer or token_number + spaces_no + 1 == answer):\n",
    "            match = True\n",
    "            break\n",
    "\n",
    "        # if (token_number + spaces_no == answer):\n",
    "        #     no_match = start_position\n",
    "        #     print('HI 1')\n",
    "        #     break\n",
    "        elif next_word == ' ' and token_number + spaces_no + 1 == answer:\n",
    "            no_match = start_position\n",
    "            break\n",
    "        if \"'\" in k_words_context_:\n",
    "            token_number += len(k_words_context_[0]) + 1\n",
    "        else:\n",
    "            token_number += len(k_words_context_[0])\n",
    "    if match:\n",
    "        return start_position\n",
    "    return no_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "id": "9c4de604-25c5-43f3-8a4b-0ba0836ddf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = [\"$\", \",\", \"''\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"``\"]\n",
    "\n",
    "\n",
    "def get_proc_pos_between_item(pos_context, context, answers):\n",
    "    positions = {}\n",
    "    copy_answer = []\n",
    "    i = 0\n",
    "    for answer in answers[\"answer_start\"]:\n",
    "        answer_position = find_start_position(answers[\"text\"][i], pos_context, answer, context)\n",
    "        if answer_position != -1:\n",
    "            positions[answer_position] = answers[\"answer_start\"][i]\n",
    "            copy_answer.append(answers[\"answer_start\"][i])\n",
    "        i += 1\n",
    "    pos = \"\"\n",
    "    offset = 0\n",
    "    offsets = {}\n",
    "    for index in range(len(pos_context)):\n",
    "        pos_text = pos_context[index][str(index)]\n",
    "\n",
    "        if pos_text[1] in special_characters:\n",
    "            pos_text[1] = \"sym\"\n",
    "        if index in positions.keys():\n",
    "            offsets[positions[index]] = offset\n",
    "        pos += f\"{pos_text[0]} [{pos_text[1].lower()}] \"\n",
    "        offset += len(f\" [{pos_text[1]}] \")\n",
    "\n",
    "    answer_starts = []\n",
    "\n",
    "    for index in range(len(copy_answer)):\n",
    "        answer = copy_answer[index]\n",
    "        spaces_no = context[:answer].count(\" \") + 1\n",
    "        answer += offsets[answer] - spaces_no\n",
    "        if pos[answer : answer + 1] == \" \":\n",
    "            answer += 1\n",
    "        # if len(answers[\"text\"]) > index:\n",
    "        #     if len(answers[\"text\"][index]) > 1:\n",
    "        #         if pos[answer][0] == answers[\"text\"][index][1]:\n",
    "        #             answer -= 1\n",
    "        answer_starts.append(answer)\n",
    "\n",
    "    return pos, answer_starts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "id": "b3417a4c-78be-44f2-bde4-cede73a22d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Super [nnp] Bowl [nnp] 50 [cd] was [vbd] an [dt] American [jj] football [nn] game [nn] to [to] determine [vb] the [dt] champion [nn] of [in] the [dt] National [nnp] Football [nnp] League [nnp] ( [hyph] NFL [nnp] ) [sym] for [in] the [dt] 2015 [cd] season [nn] . [sym] The [dt] American [nnp] Football [nnp] Conference [nnp] ( [sym] AFC [nnp] ) [sym] champion [nn] Denver [nnp] Broncos [nnps] defeated [vbd] the [dt] National [nnp] Football [nnp] Conference [nnp] ( [sym] NFC [nnp] ) [sym] champion [nn] Carolina [nnp] Panthers [nnps] 24 [cd] – [sym] 10 [cd] to [to] earn [vb] their [prp$] third [jj] Super [nnp] Bowl [nnp] title [nn] . [sym] The [dt] game [nn] was [vbd] played [vbn] on [in] February [nnp] 7 [cd] , [sym] 2016 [cd] , [sym] at [in] Levi [nnp] \\'s [pos] Stadium [nnp] in [in] the [dt] San [nnp] Francisco [nnp] Bay [nnp] Area [nnp] at [in] Santa [nnp] Clara [nnp] , [sym] California [nnp] . [sym] As [in] this [dt] was [vbd] the [dt] 50th [jj] Super [nnp] Bowl [nnp] , [sym] the [dt] league [nn] emphasized [vbd] the [dt] \" [sym] golden [jj] anniversary [nn] \" [sym] with [in] various [jj] gold-themed [jj] initiatives [nns] , [sym] as [rb] well [rb] as [in] temporarily [rb] suspending [vbg] the [dt] tradition [nn] of [in] naming [vbg] each [dt] Super [nnp] Bowl [nnp] game [nn] with [in] Roman [jj] numerals [nns] ( [sym] under [in] which [wdt] the [dt] game [nn] would [md] have [vb] been [vbn] known [vbn] as [in] \" [sym] Super [nnp] Bowl [nnp] L [nnp] \") [sym] , [sym] so [in] that [in] the [dt] logo [nn] could [md] prominently [rb] feature [vb] the [dt] Arabic [jj] numerals [nns] 50 [cd] . [sym] '"
      ]
     },
     "execution_count": 1162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_id = 1 \n",
    "\n",
    "pos, answer_starts = get_proc_pos_between_item(data[example_id]['POS_context'], data[example_id]['context'],\n",
    "                          data[example_id]['answers'])\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "id": "db67f36b-bc91-4862-9595-96f239066f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [00:16<00:00, 622.62it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm.tqdm(data):\n",
    "    context = item['context']\n",
    "    question = item['question']\n",
    "    item['context'], item['answers']['answer_start'] = get_proc_pos_between_item(item['POS_context'], context, item['answers'])\n",
    "\n",
    "    item.pop('POS_context', None)\n",
    "    item.pop('POS_question', None)\n",
    "    item.pop('NER_question', None)\n",
    "    item.pop('NER_context', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "id": "0f2c6656-d151-4d15-8b87-9011e1ea9e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10570it [00:00, 250941.26it/s]\n"
     ]
    }
   ],
   "source": [
    "incorrect_answers_no = 0\n",
    "\n",
    "for index, line in tqdm.tqdm(enumerate(data)):\n",
    "    try:\n",
    "        if len(line[\"answers\"][\"answer_start\"]) == 0:\n",
    "            continue\n",
    "        answer_start = line[\"answers\"][\"answer_start\"][0]\n",
    "        answer = line[\"answers\"][\"text\"][0]\n",
    "\n",
    "        if answer_start == -1:\n",
    "            continue\n",
    "\n",
    "        if line[\"context\"][answer_start:][0] != answer[0]:\n",
    "            # line[\"answers\"][\"answer_start\"][0] -= 1\n",
    "            incorrect_answers_no += 1\n",
    "            #print(index, line[\"context\"][answer_start:], 4*'-', answer, answer_start)\n",
    "\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "id": "bbf7901c-ebc4-4eeb-acdf-c05dc8a82d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with 'Test' property added has been saved to ./squad_data_validation_pos_between.json\n"
     ]
    }
   ],
   "source": [
    "write_data(\"./squad_data_validation_pos_between.json\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "id": "86266d76-7aa2-43fe-8e61-a76289dfbdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data = read_data('./squad_data_validation_pos_between.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "id": "60a6fffe-eaff-41a2-877b-a1fe305eb5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rreplace(string, substring, replacement):\n",
    "    k = string.rfind(substring)\n",
    "    return string[:k] + replacement + string[k+len(substring):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "id": "515b8a84-f894-4433-b5c2-2d7d1669f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = [\"'\", '.', ',', ')']\n",
    "\n",
    "\n",
    "def get_updated_answer(context, answers):\n",
    "    new_answers = []\n",
    "    if len(answers['answer_start']) == 0:\n",
    "        return\n",
    "\n",
    "    for index, answer in enumerate(answers['text']):\n",
    "        new_answer = ''\n",
    "        if len(answers['answer_start']) < index + 1:\n",
    "            return new_answers\n",
    "        answer_start = answers['answer_start'][index]\n",
    "        if answer_start != -1:\n",
    "            tokenized_context = Sentence(context[answer_start:])\n",
    "            no_of_words = len(Sentence(answer))\n",
    "            max_length = no_of_words * 3 + no_of_words\n",
    "            if max_length > len(tokenized_context):\n",
    "                max_length = len(tokenized_context)\n",
    "            minus_words = 0\n",
    "            for token_index in range(max_length):\n",
    "                # if token_index == max_length - minus_words - 1:\n",
    "                #     break\n",
    "                if (tokenized_context[token_index].text == '[' and tokenized_context[token_index+1].text != '[') or (len(tokenized_context) > token_index+1 and tokenized_context[token_index].text!=']' and tokenized_context[token_index+1].text==']') or token_index == no_of_words * 4 - 1:\n",
    "                    new_answer += tokenized_context[token_index].text\n",
    "                elif tokenized_context[token_index].text in symbols and tokenized_context[token_index+1].text != '[':\n",
    "                    new_answer += tokenized_context[token_index].text\n",
    "                elif (tokenized_context[token_index].text.replace('.', '').isalpha() or tokenized_context[token_index].text[-1] in '.\"') and tokenized_context[token_index+1].text in '.$]':\n",
    "                    new_answer += tokenized_context[token_index].text\n",
    "                    # minus_words += 1\n",
    "                elif len(tokenized_context) > token_index+1 and tokenized_context[token_index].text.replace('.', '').isnumeric() and tokenized_context[token_index+1].text in '.$]':\n",
    "                    new_answer += tokenized_context[token_index].text\n",
    "                elif tokenized_context[token_index].text.replace('-', '').isalpha() and tokenized_context[token_index+1].text in '.$]':\n",
    "                    new_answer += tokenized_context[token_index].text\n",
    "                elif len(tokenized_context) > token_index+1 and tokenized_context[token_index+1].text[0].isnumeric() and tokenized_context[token_index].text in '-':\n",
    "                    new_answer += tokenized_context[token_index].text\n",
    "                else:\n",
    "                    new_answer += tokenized_context[token_index].text + ' '\n",
    "        new_answer = new_answer.rstrip()\n",
    "        if answer_start != -1:\n",
    "            tokens_new_answer = Sentence(new_answer)\n",
    "            # print(tokens_new_answer[-2])\n",
    "            if tokens_new_answer[-2].text == '[':\n",
    "                # print(tokens_new_answer[-2])\n",
    "                new_answer += ']'\n",
    "            if tokens_new_answer[-1].text == '[':\n",
    "                #print(tokens_new_answer[-1], tokens_new_answer[-2].text)\n",
    "                new_answer = rreplace(new_answer, tokens_new_answer[-1].text, '')\n",
    "                new_answer = rreplace(new_answer, tokens_new_answer[-2].text, '')\n",
    "                new_answer = new_answer.rstrip()\n",
    "        new_answers.append(new_answer)\n",
    "    return new_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "id": "c75fcb27-e8cf-4951-bfe0-33133336a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = 4892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "id": "b9de6e1d-817f-4a8d-8028-e30b65562bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dial-up [jj] terminal [nn] to [in] a [dt] PAD [nn] , [sym] or [cc] , [sym] by [in] linking [vbg] a [dt] permanent [jj] X.25 [nn] node [nn] to [in] the [dt] network. [nn] [ [sym] citation [nn] needed [vbn] ] [sym]',\n",
       " 'dial-up [jj] terminal [nn] to [in] a [dt] PAD [nn] , [sym] or [cc] , [sym] by [in] linking [vbg] a [dt] permanent [jj] X.25 [nn] node [nn]']"
      ]
     },
     "execution_count": 1275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_updated_answer(proc_data[example_id]['context'], proc_data[example_id]['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "id": "41636afb-407f-423a-9827-99823de52d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [01:45<00:00, 100.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm.tqdm(proc_data):\n",
    "    item['answers']['text'] = get_updated_answer(item['context'], item['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "id": "19b126a5-8deb-4467-8a91-a4b3fe2dd04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10570it [00:00, 343680.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 New [nnp] Orleans' [nnp] Mercedes-Benz [nnp] Superdome [nnp] , [sym] Miami [nnp] 's [pos] Sun [nnp] Life [nnp] Stadium [nnp] , [sym] and [cc] the [dt] San [nnp] Francisco [nnp] Bay [nnp] Area [nnp] 's [pos] Levi [nnp] 's [pos] Stadium ---- New [nnp] Orleans' [nnp] Mercedes-Benz [nnp] Superdome [nnp] , [sym] Miami [nnp] 's [pos] Sun [nnp] Life [nnp] Stadium [nnp] , [sym] and [cc] the [dt] San [nnp] Francisco [nnp] Bay [nnp] Area [nnp] 's [pos] Levi [nnp] 's [pos] Stadium\n",
      "3529 [cd] ⁄ [sym] 3 [cd] normal ---- [cd] ⁄ [sym] 3 [cd] normal\n",
      "3631 [cd] billion [cd] years [nns] ago [rb] during ---- [cd] billion [cd] years [nns] ago [rb] during\n",
      "4967 Y. [nnp] p. [nn] orientalis [nn] and [cc] Y. [nnp] p. [nn] medievalis ---- Y. [nnp] p. [nn] orientalis [nn] and [cc] Y. [nnp] p. [nn] medievalis\n",
      "7145 [prp] was [vbd] never [rb] affiliated [vbn] with [in] any [dt] particular [jj] denomination [nn] , ---- [prp] was [vbd] never [rb] affiliated [vbn] with [in] any [dt] particular [jj] denomination [nn] ,\n",
      "8643 Michael [nnp] E. [nnp] Mann [nnp] , [sym] Raymond [nnp] S. [nnp] Bradley [nnp] and [cc] Malcolm [nnp] K. [nnp] Hughes ---- Michael [nnp] E. [nnp] Mann [nnp] , [sym] Raymond [nnp] S. [nnp] Bradley [nnp] and [cc] Malcolm [nnp] K. [nnp] Hughes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "incorrect_answers_no = 0\n",
    "\n",
    "for index, line in tqdm.tqdm(enumerate(proc_data)):\n",
    "    try:\n",
    "        if len(line[\"answers\"][\"answer_start\"]) == 0:\n",
    "            continue\n",
    "        answer_start = line[\"answers\"][\"answer_start\"][0]\n",
    "        answer = line[\"answers\"][\"text\"][0]\n",
    "        answer_length = len(answer)\n",
    "\n",
    "        if answer_start == -1:\n",
    "            continue\n",
    "\n",
    "        if line[\"context\"][answer_start: answer_start + answer_length] != answer:\n",
    "            incorrect_answers_no += 1\n",
    "            print(index, line[\"context\"][answer_start: answer_start + answer_length],\n",
    "                  \"-\" * 4, answer)\n",
    "\n",
    "        # if answer[-1] != ']':\n",
    "        #     print(index, line[\"context\"][answer_start: answer_start + answer_length], \"-\" * 4, answer)\n",
    "        #     incorrect_answers_no += 1\n",
    "\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "id": "2132257a-5271-4437-b54e-86578735ea8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056764427625354774"
      ]
     },
     "execution_count": 1280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_answers_no / len(proc_data) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "id": "c4fdc6ac-65fb-4c8e-8ab8-58ce5a86a3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with 'Test' property added has been saved to ./squad_data_validation_pos_ner_answer_updated.json\n"
     ]
    }
   ],
   "source": [
    "write_data(\"./squad_data_validation_pos_ner_answer_updated.json\", proc_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
