{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2416e8e8",
   "metadata": {},
   "source": [
    "### 0. Import libraries and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e8ca0b-2e9c-4884-8d26-a1cdf557a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import tqdm\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from flair.data import Sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6533324-85cf-4079-a104-d20a981f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_file_path) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58fc9b90-51ea-463b-87d1-128d5ad44cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(output_file_path, data):\n",
    "    # Open a new JSON file for writing\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        for data_line in data:\n",
    "            output_file.write(json.dumps(data_line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae87b0ef-e953-4e00-ac30-890583a094dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"../../data/squad_data_train_pos_ner_agg.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea924d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_TAGS = ['[LOC]', '[MISC]', '[ORG]', '[PER]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c484e",
   "metadata": {},
   "source": [
    "### 1. Find the token index where the answer starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4232297f-f49b-4f17-9fe9-5c04c6960153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subustring_in_string(string, substring, symbols):\n",
    "    for symbol in symbols:\n",
    "        if symbol in string and (substring in string or substring.replace(\" \", \"\") in string.replace(\" \", \"\")):\n",
    "           return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f685db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_one_text_in_context(text, context, symbols):\n",
    "    count = 0\n",
    "\n",
    "    for symbol in symbols:\n",
    "        if context.count(text + symbol):\n",
    "            count += 1\n",
    "\n",
    "    return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f02270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_start_position(text, pos_context, answer, context):\n",
    "    tokens = []\n",
    "    sentence = Sentence(text)\n",
    "    symbols = [\" \", \")\", \"(\", \"%\", \"-\", \".\", \",\"]\n",
    "\n",
    "    # get how many spaces are until the start position\n",
    "    spaces_no = context[:answer].count(\" \")\n",
    "\n",
    "    # get tokens of the answer\n",
    "    for word in sentence:\n",
    "        tokens.append(word.text)\n",
    "\n",
    "    start_position = 0\n",
    "    token_number = 0\n",
    "    match = False\n",
    "    no_match = -1\n",
    "\n",
    "    # check if the answer appear only once\n",
    "    one_apperance = is_one_text_in_context(text, context, symbols)\n",
    "\n",
    "    # iterate over positions in the pos context\n",
    "    for start_position in range(len(pos_context) - len(tokens) + 1):\n",
    "        # store in k_words_context_ and k_words_context the span of tokens from pos_context\n",
    "        k_words_context_ = [\n",
    "            bytes(list(line.values())[0][0], \"utf-8\").decode(\"unicode_escape\")\n",
    "            for line in pos_context[start_position : start_position + len(tokens)]\n",
    "        ]\n",
    "\n",
    "        k_words_context = [\n",
    "            list(line.values())[0][0].replace(\"\\\\\", \"\")\n",
    "            for line in pos_context[start_position : start_position + len(tokens)]\n",
    "        ]\n",
    "\n",
    "        # get the next word after the context\n",
    "        next_word = [\n",
    "            list(line.values())[0][0]\n",
    "            for line in pos_context[\n",
    "                start_position + len(tokens) : start_position + len(tokens) + 1\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "\n",
    "        # join all words in a string\n",
    "        k_words_context_string = \" \".join(k_words_context)\n",
    "        k_words_context_string_ = \" \".join(k_words_context_)\n",
    "\n",
    "        # define exact mach condition\n",
    "        exact_match = k_words_context == tokens or k_words_context_ == tokens\n",
    "\n",
    "\n",
    "        # if there is only one appearence and the answer was found, stop\n",
    "        if context.count(text) == 1 and exact_match:\n",
    "            match = True\n",
    "            break\n",
    "\n",
    "        # if there is only one appearence with the answer followed by a token, stop\n",
    "        if one_apperance and (\n",
    "            exact_match\n",
    "            or is_subustring_in_string(k_words_context_string, text, symbols)\n",
    "            or is_subustring_in_string(k_words_context_string_, text, symbols)\n",
    "        ):\n",
    "            match = True\n",
    "            break\n",
    "\n",
    "        # if the occurence is not the first one, but the start position is correct, stop\n",
    "        if (\n",
    "            text in k_words_context_string\n",
    "            or text in k_words_context_string_\n",
    "            or exact_match\n",
    "            or is_subustring_in_string(k_words_context_string, text, symbols)\n",
    "            or is_subustring_in_string(k_words_context_string_, text, symbols)\n",
    "        ) and (\n",
    "            token_number + spaces_no == answer or token_number + spaces_no + 1 == answer or token_number + spaces_no - 1 == answer\n",
    "        ):\n",
    "            match = True\n",
    "            break\n",
    "\n",
    "        # else consider the next word\n",
    "        elif next_word in symbols and (token_number + spaces_no == answer or token_number + spaces_no + 1 == answer or token_number + spaces_no - 1 == answer):\n",
    "            no_match = start_position\n",
    "            break\n",
    "\n",
    "        if \"'\" in k_words_context_:\n",
    "            token_number += len(k_words_context_[0])\n",
    "        else:\n",
    "            token_number += len(k_words_context[0])\n",
    "\n",
    "    if match:\n",
    "        return start_position\n",
    "    \n",
    "    return no_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(line):\n",
    "    original_answer = line[\"answers\"][\"text\"][0]\n",
    "    original_answer_start = line[\"answers\"][\"answer_start\"][0]\n",
    "    original_answer_list = [token.text for token in list(Sentence(original_answer))]\n",
    "\n",
    "    index = find_start_position(original_answer, line[\"POS_context\"], original_answer_start, line[\"context\"])\n",
    "    found_answer = [list(token.values())[0][0] for token in line['POS_context'][index:index+len(original_answer_list)]]\n",
    "\n",
    "    return original_answer_list, found_answer, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc40ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_start_answer_detection(data):\n",
    "    incorrect_lines_indices = []\n",
    "    \n",
    "    for i, line in tqdm.tqdm(enumerate(data)):\n",
    "        original_answer_list, found_answer, index = extract_answer(line)\n",
    "\n",
    "        if not \" \".join(found_answer).startswith(\" \".join(original_answer_list)):\n",
    "            incorrect_lines_indices.append(i)\n",
    "\n",
    "    return incorrect_lines_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f4233",
   "metadata": {},
   "source": [
    "### 2. Insert NER in the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96036c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_ne_tags(line):\n",
    "    \"\"\"\n",
    "    Find start index for answer\n",
    "    \"\"\"\n",
    "    original_answer_list, found_answer, index = extract_answer(line)\n",
    "\n",
    "    \"\"\"\n",
    "    Get spans of continuous indices\n",
    "    \"\"\"\n",
    "    # get tokens\n",
    "    tokens = [list(token.values())[0][0] for token in line[\"POS_context\"]]\n",
    "\n",
    "    continuous_spans_indices = [\n",
    "        list(map(itemgetter(1), g))\n",
    "        for _, g in groupby(\n",
    "            enumerate([int(list(token.keys())[0]) for token in line[\"NER_context\"]]),\n",
    "            lambda i_x: i_x[0] - i_x[1],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    token_index_pos_map = {int(list(token.keys())[0]):index for index, token in enumerate(line[\"NER_context\"])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Insert NE tags and find the new start index for answer\n",
    "    \"\"\"\n",
    "    new_index = 0\n",
    "    updated_context = []\n",
    "    span_index = 0\n",
    "    token_index = 0\n",
    "    initial_token_index = 0\n",
    "\n",
    "    while token_index < len(tokens):\n",
    "        try:\n",
    "            continuous_spans_indices[span_index][0]\n",
    "        except:\n",
    "            updated_context.append(tokens[token_index])\n",
    "            if initial_token_index == index:\n",
    "                new_index = len(updated_context) - 1\n",
    "\n",
    "            token_index += 1\n",
    "            initial_token_index += 1\n",
    "            continue\n",
    "\n",
    "        if token_index + 1 > continuous_spans_indices[span_index][0] and len(continuous_spans_indices[span_index]) == 1:\n",
    "            ne_tags = [tag[1] for tag in list(line[\"NER_context\"][token_index_pos_map[token_index]].values())[0]]\n",
    "\n",
    "            for ne_tag in ne_tags:\n",
    "                updated_context.append(\"[\" + ne_tag + \"]\")\n",
    "\n",
    "            updated_context.append(tokens[token_index])\n",
    "\n",
    "            if initial_token_index == index:\n",
    "                \n",
    "                if updated_context[-2] in NER_TAGS:\n",
    "                    new_index = len(updated_context) - 1 - len(ne_tags)\n",
    "                else:\n",
    "                    new_index = len(updated_context) - 1\n",
    "\n",
    "            for ne_tag in ne_tags[::-1]:\n",
    "                updated_context.append(\"[\" + ne_tag + \"]\")\n",
    "\n",
    "            span_index += 1\n",
    "            token_index += 1\n",
    "            initial_token_index += 1\n",
    "\n",
    "\n",
    "        elif token_index + 1 > continuous_spans_indices[span_index][0] and len(continuous_spans_indices[span_index]) > 1:\n",
    "            ne_tags = [tag[1] for tag in list(line[\"NER_context\"][token_index_pos_map[token_index]].values())[0]]\n",
    "\n",
    "            for ne_tag in ne_tags:\n",
    "                updated_context.append(\"[\" + ne_tag + \"]\")\n",
    "\n",
    "            for i in range(len(continuous_spans_indices[span_index])):\n",
    "                updated_context.append(tokens[token_index])\n",
    "\n",
    "                if initial_token_index == index:\n",
    "                    if updated_context[-2] in NER_TAGS:\n",
    "                        new_index = len(updated_context) - 1 - len(ne_tags)\n",
    "                    else:\n",
    "                        new_index = len(updated_context) - 1\n",
    "\n",
    "                token_index += 1\n",
    "                initial_token_index += 1\n",
    "\n",
    "            for ne_tag in ne_tags[::-1]:\n",
    "                updated_context.append(\"[\" + ne_tag + \"]\")\n",
    "\n",
    "            span_index += 1\n",
    "\n",
    "        else:\n",
    "            updated_context.append(tokens[token_index])\n",
    "            if initial_token_index == index:\n",
    "                new_index = len(updated_context) - 1\n",
    "\n",
    "            token_index += 1\n",
    "            initial_token_index += 1\n",
    "\n",
    "        if token_index == len(tokens) - 1:\n",
    "            updated_context.append(tokens[token_index])\n",
    "            if initial_token_index == index:\n",
    "                new_index = len(updated_context) - 1\n",
    "\n",
    "    return updated_context, new_index, original_answer_list, tokens, index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4fb6e",
   "metadata": {},
   "source": [
    "### 3. Find last index of the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dbcd025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_index(updated_context, new_index, original_answer_list, tokens, index):\n",
    "    \"\"\"\n",
    "    Find last index of answer in the updated context\n",
    "    \"\"\"\n",
    "    new_last_index = new_index\n",
    "    searched_answer = copy.deepcopy(original_answer_list)\n",
    "\n",
    "    searching_answer_iter = -1\n",
    "\n",
    "    while len(searched_answer) != 0 or updated_context[new_last_index] in NER_TAGS:\n",
    "        searching_answer_iter += 1\n",
    "\n",
    "        if len(searched_answer) != 0:\n",
    "            if searched_answer[0] in updated_context[new_last_index]:\n",
    "                searched_answer.pop(0)\n",
    "                new_last_index += 1\n",
    "\n",
    "        try:\n",
    "            if updated_context[new_last_index] in NER_TAGS:\n",
    "                new_last_index += 1\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        if new_last_index == len(updated_context) or searching_answer_iter == 100:\n",
    "            break\n",
    "\n",
    "    original_answer = tokens[index:index+len(original_answer_list)]\n",
    "    updated_answer = updated_context[new_index:new_last_index]\n",
    "\n",
    "    if original_answer != [token for token in updated_answer if token not in NER_TAGS]:\n",
    "        return -1\n",
    "    \n",
    "    return new_last_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee80371",
   "metadata": {},
   "source": [
    "### 4. Process each line and update the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87599it [01:24, 1038.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines for which the start index was not detected correctly: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "incorrect_lines_indices = check_start_answer_detection(data)\n",
    "print(f\"Number of lines for which the start index was not detected correctly: {len(incorrect_lines_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases in which the start index was not detected correctly:\n",
      "\n",
      "['Father', 'Joseph', 'Carrier', ',', 'C.S.C', '.'] ['Father', 'Joseph', 'Carrier', ',', 'C.S.C.', 'was'] 0\n",
      "['Rev', '.', 'John', 'J.', 'Cavanaugh', ',', 'C.S.C', '.'] ['The', 'Rev', '.', 'John', 'J.', 'Cavanaugh', ',', 'C.S.C.'] 0\n",
      "['five', '.'] ['top-five', '.'] 149\n",
      "['Jay', 'Z', '.'] ['married', 'Jay', 'Z.'] 7\n",
      "[\"'\", '03', 'Bonnie', '&', 'Clyde'] ['\"\\'', '03', 'Bonnie', '&', 'Clyde'] 16\n",
      "['B.I.C', '.'] ['\"', 'B.I.C.'] 91\n",
      "['B.I.C', '.'] ['\"', 'B.I.C.'] 91\n",
      "['50'] [] -1\n",
      "['J.', 'S.', 'Bach', ',', 'Mozart', 'and', 'Schubert'] [] -1\n",
      "['J.', 'S.', 'Bach', ',', 'Mozart', 'and', 'Schubert'] [] -1\n",
      "['J.', 'S.', 'Bach', ',', 'Mozart', 'and', 'Schubert'] [] -1\n",
      "['7'] [] -1\n",
      "['7'] [] -1\n",
      "['7'] [] -1\n",
      "['Rondo', 'Op', '.', '1', '.'] ['his', 'Rondo', 'Op', '.', '1.'] 162\n",
      "['the', 'Canuts', '.'] [',', 'the', 'Canuts.'] 42\n",
      "['J.S', '.', 'Bach', \"'s\", 'The', 'Well-Tempered', 'Clavier'] ['by', 'J.S.', 'Bach', \"'s\", 'The', 'Well-Tempered', 'Clavier'] 38\n",
      "['Op', '.', '58'] [',', 'Op.', '58'] 18\n",
      "['Tibet'] ['Sino-Tibetan'] 34\n",
      "['3'] ['23'] 27\n",
      "['one'] [] -1\n",
      "['2015'] [] -1\n",
      "['5'] [] -1\n",
      "['2007'] ['mid-2007'] 2\n",
      "['DRM'] [] -1\n",
      "['hundred', 'million'] [] -1\n",
      "['four'] [] -1\n",
      "['Warhead', '2000', 'A.D', '.'] ['entitled', 'Warhead', '2000', 'A.D.'] 170\n",
      "['$', '9.2', 'million'] ['($', '9.2', 'million'] 26\n",
      "['because', 'it', 'was', 'caused', 'by', 'a', 'different', 'fault', '.'] ['series', 'because', 'it', 'was', 'caused', 'by', 'a', 'different', 'fault.'] 18\n",
      "['700'] ['1,700'] 27\n",
      "['1', 'million'] ['11', 'million'] 19\n",
      "['William', 'F.', 'Buckley', ',', 'Jr', '.'] ['by', 'William', 'F.', 'Buckley', ',', 'Jr.'] 49\n",
      "['50'] ['550'] 43\n",
      "['92'] [] -1\n",
      "['65,000'] ['865,000'] 45\n",
      "['$', '15,887'] ['($', '15,887'] 125\n",
      "['J.', 'B.', 'Lippincott'] [] -1\n",
      "['t', 'located', '15', 'km', 'southwest', 'of', 'Dushanbe'] ['airport', 'located', '15', 'km', 'southwest', 'of', 'Dushanbe'] 81\n",
      "['culturally', 'specific', \"'\", \"aesthetics'\"] [] -1\n",
      "['7'] [] -1\n",
      "['7'] [] -1\n",
      "['DJ', 'No', 'I.D', '.'] ['/', 'DJ', 'No', 'I.D.'] 166\n",
      "['No', 'I.D', '.'] ['No', 'I.D.', ','] 168\n",
      "['3'] [] -1\n",
      "['3'] [] -1\n",
      "['6'] ['3,086,000'] 75\n",
      "['6'] ['3,086,000'] 75\n",
      "['\"', 'suffering', '\"', ',', '\"', 'anxiety', '\"', ',', '\"', 'unsatisfactoriness', '\"', ',', '\"', 'unease', '\"', ',', 'etc', '.'] ['as', '\"', 'suffering', '\"', ',', '\"', 'anxiety', '\"', ',', '\"', 'unsatisfactoriness', '\"', ',', '\"', 'unease', '\"', ',', 'etc.'] 13\n",
      "['the', 'continuation', 'of', 'the', 'cycle', 'of', 'suffering', 'and', 'rebirth', '(', 'saṃsāra', ')', 'in', 'detail', '.'] ['the', 'continuation', 'of', 'the', 'cycle', 'of', 'suffering', 'and', 'rebirth', '(', 'saṃsāra', ')', 'in', 'detail.', '['] 33\n",
      "['samādhi'] [] -1\n",
      "['ritual'] [] -1\n",
      "['15'] [] -1\n",
      "['Harry', 'Connick', ',', 'Jr', '.'] ['mentor', 'Harry', 'Connick', ',', 'Jr.'] 55\n",
      "['15'] [] -1\n",
      "['5'] [] -1\n",
      "['15'] [] -1\n",
      "['9'] ['1990s'] 46\n",
      "['8'] ['18'] 14\n",
      "['9'] [] -1\n",
      "['8'] [] -1\n",
      "['one', 'of', 'the', 'most', 'successful', 'species', 'on', 'the', 'planet', 'today', '.'] ['one', 'of', 'the', 'most', 'successful', 'species', 'on', 'the', 'planet', 'today.', ':'] 70\n",
      "['2'] ['72'] 29\n",
      "['athletes'] ['non-athletes'] 170\n",
      "['lanterns', '.'] ['8', 'lanterns.'] 51\n",
      "['rld', 'around', 'it'] ['world', 'around', 'it'] 33\n",
      "['00', 'of', 'which', 'was', 'paid', 'by', 'Cambridge', 'University', 'Press', ',', '200', 'by', 'the', 'Royal', 'Society', 'of', 'London', ',', 'and', '50', 'apiece', 'by', 'Whitehead', 'and', 'Russell'] ['300', 'of', 'which', 'was', 'paid', 'by', 'Cambridge', 'University', 'Press', ',', '200', 'by', 'the', 'Royal', 'Society', 'of', 'London', ',', 'and', '50', 'apiece', 'by', 'Whitehead', 'and', 'Russell'] 70\n",
      "['recession', 'in', 'the', 'U.S', '.'] ['a', 'recession', 'in', 'the', 'U.S.'] 62\n",
      "['\"', 'Has', 'Financial', 'Development', 'Made', 'the', 'World', 'Riskier', '?', '\"'] [',', '\"', 'Has', 'Financial', 'Development', 'Made', 'the', 'World', 'Riskier', '?\"'] 64\n",
      "['September', '2007'] ['mid-September', '2007'] 45\n",
      "['−', '1.0', '%'] ['(−', '1.0', '%)'] 48\n",
      "['2', '%'] ['12', '%'] 104\n",
      "['16'] ['under-16-year-olds'] 23\n",
      "['Netbula', ',', 'LLC', 'v', '.', 'Chordiant', 'Software', 'Inc', '.'] [',', 'Netbula', ',', 'LLC', 'v', '.', 'Chordiant', 'Software', 'Inc.'] 4\n",
      "['symbiosis'] [] -1\n",
      "['Oak', 'Productions', ',', 'Inc', '.'] ['called', 'Oak', 'Productions', ',', 'Inc.'] 151\n",
      "['7', '%'] [] -1\n",
      "['30s'] [] -1\n",
      "['Plymouth', 'Argyle', 'F.C', '.'] ['to', 'Plymouth', 'Argyle', 'F.C.'] 3\n",
      "['Vospers', 'Oak', 'Villa', 'F.C', '.'] [',', 'Vospers', 'Oak', 'Villa', 'F.C.'] 93\n",
      "['Plymouth', 'Albion', 'R.F.C', '.'] ['include', 'Plymouth', 'Albion', 'R.F.C.'] 3\n",
      "['\"', 'what', 'does', 'reality', 'consist', 'of', '?', '\"'] [':', '\"', 'what', 'does', 'reality', 'consist', 'of', '?\"'] 70\n",
      "['2'] [] -1\n",
      "['19th'] ['mid-19th'] 2\n",
      "['L.A', '.', 'Reid'] ['by', 'L.A.', 'Reid'] 40\n",
      "['5'] [] -1\n",
      "['1985'] ['post-1985'] 78\n",
      "['G.', 'Keith', 'Bryant'] [] -1\n",
      "['men'] [] -1\n",
      "['Secretary', 'of', 'State'] ['then-Secretary', 'of', 'State'] 4\n",
      "['91'] ['1917'] 49\n",
      "['J.', 'Cheever', 'Cowdin'] [] -1\n",
      "['J.', 'Cheever', 'Cowdin'] [] -1\n",
      "['U.S.', 'vs.', 'Paramount', 'Pictures', ',', 'et', 'al', '.'] ['1948', 'U.S.', 'vs.', 'Paramount', 'Pictures', ',', 'et', 'al.'] 34\n",
      "['1962'] ['mid-1962'] 13\n",
      "['Universal', 'City', 'Studios', ',', 'Inc', '.'] ['formed', 'Universal', 'City', 'Studios', ',', 'Inc.'] 56\n",
      "['Edgar', 'Bronfman', 'Jr', '.'] ['head', 'Edgar', 'Bronfman', 'Jr.'] 5\n",
      "['Universal', 'Productions', 'France', 'S.A', '.'] [',', 'Universal', 'Productions', 'France', 'S.A.'] 30\n",
      "['U.S', '.'] ['first', 'U.S.'] 58\n",
      "['9'] [] -1\n",
      "['3'] [] -1\n",
      "['4'] [] -1\n",
      "['3'] ['23'] 23\n",
      "['1980s'] ['mid-1980s'] 52\n",
      "['1'] [] -1\n",
      "['3'] ['230'] 57\n",
      "['p'] [] -1\n",
      "['aspirated'] [] -1\n",
      "['negative'] [] -1\n",
      "['1920s'] ['mid-1920s'] 91\n",
      "['19th', 'century'] ['mid-19th', 'century'] 60\n",
      "['20th', 'century'] ['mid-20th', 'century'] 4\n",
      "['2', 'million'] ['1.2', 'million'] 58\n",
      "['mid', '2015'] ['In-mid', '2015'] 0\n",
      "['5'] ['35'] 30\n",
      "['in'] [] -1\n",
      "['16'] [] -1\n",
      "['St.', 'Louis', 'Rams'] ['Then-St.', 'Louis', 'Rams'] 10\n",
      "['16'] [] -1\n",
      "['1950s'] ['mid-1950s'] 24\n",
      "['1970s'] ['mid-1970s'] 22\n",
      "['1970s'] ['mid-1970s'] 20\n",
      "['D.H.T', '.'] ['by', 'D.H.T.'] 45\n",
      "['2000s'] ['mid-2000s'] 2\n",
      "['£', '1,310,000', 'to', '£', '1,530,000'] ['(£', '1,310,000', 'to', '£', '1,530,000'] 17\n",
      "['ks'] [] -1\n",
      "['n'] [] -1\n",
      "['v'] [] -1\n",
      "['3,000'] ['13,000'] 71\n",
      "['8'] [] -1\n",
      "['.', '6', '%'] ['of', '0.6', '%'] 33\n",
      "['.', '45', '%'] ['was', '0.45', '%'] 156\n",
      "['.', '25', '%'] ['about', '0.25', '%'] 101\n",
      "['59'] ['259'] 25\n",
      "['sere'] [] -1\n",
      "['The', 'East', 'Slavs', 'colonised', 'Siberia', 'and', 'Central', 'Asia', '.'] ['.', 'The', 'East', 'Slavs', 'colonised', 'Siberia', 'and', 'Central', 'Asia.'] 83\n",
      "['he', 'word', 'slovo', '(\"', 'word', '\")', 'and', 'the', 'related', 'slava', '(\"', 'fame', '\")', 'and', 'slukh', '(\"', 'hearing', '\")'] ['The', 'word', 'slovo', '(\"', 'word', '\")', 'and', 'the', 'related', 'slava', '(\"', 'fame', '\")', 'and', 'slukh', '(\"', 'hearing', '\")'] 0\n",
      "['c.', '600', 'AD'] [] -1\n",
      "['Stalin'] ['Tito-Stalin'] 1\n",
      "['U.S', '.'] [';', 'U.S.'] 90\n",
      "['U.S', '.'] [';', 'U.S.'] 90\n",
      "['1943'] ['mid-1943'] 36\n",
      "['the', 'U.S', '.'] ['gives', 'the', 'U.S.'] 9\n",
      "['about', 'the', 'fifth', 'century', '.'] ['about', 'the', 'fifth', 'century.', ':'] 161\n",
      "['Indo-European', 'origin'] ['non-Indo-European', 'origin'] 15\n",
      "['\"', 'that', 'which', 'segregates', 'and', 'recombines', 'with', 'appreciable', 'frequency', '.', '\"'] [':', '\"', 'that', 'which', 'segregates', 'and', 'recombines', 'with', 'appreciable', 'frequency.', '\"'] 63\n",
      "['3'] ['804.3'] 17\n",
      "['Japan'] ['Sino-Japanese'] 3\n",
      "['2015'] ['mid-2015'] 1\n",
      "['\"', 'no', 'HD', 'broadcasts', 'mean', 'no', 'HD', 'TVs', 'bought', 'means', 'no', 'HD', 'broadcasts', '.', '..\"'] ['of', '\"', 'no', 'HD', 'broadcasts', 'mean', 'no', 'HD', 'TVs', 'bought', 'means', 'no', 'HD', 'broadcasts', '...\"'] 24\n",
      "['£', '60', 'million'] ['(£', '60', 'million'] 54\n",
      "['C.', 'R.', 'Dodwell'] [] -1\n",
      "['(', 'clay', 'spheres', ',', 'cones', ',', 'etc', '.', ')'] ['calculi', '(', 'clay', 'spheres', ',', 'cones', ',', 'etc.', ')'] 40\n",
      "['50s'] [] -1\n",
      "['Alamin', 'M.', 'Mazrui', 'et', 'al', '.'] ['to', 'Alamin', 'M.', 'Mazrui', 'et', 'al.'] 54\n",
      "['Tory'] ['pro-Tory'] 7\n",
      "['R.K', '.', 'Khanna', 'Tennis', 'Complex'] [',', 'R.K.', 'Khanna', 'Tennis', 'Complex'] 24\n",
      "['50', 'year'] ['over-50', 'year'] 142\n",
      "['\"', 'anti', '\"', 'body', '\"', 'gen', '\"', 'erators'] ['(\"', 'anti', '\"', 'body', '\"', 'gen', '\"', 'erators'] 45\n",
      "['dard'] [] -1\n",
      "['Sisvel', 'S.p.A', '.'] ['Sisvel', 'S.p.A.', 'and'] 0\n",
      "['P.', 'J.', 'Escalante'] [] -1\n",
      "['Robert', 'Tappan', 'Morris', ',', 'Jr', '.'] ['student', 'Robert', 'Tappan', 'Morris', ',', 'Jr.'] 71\n",
      "['Zionist'] ['anti-Zionistic'] 88\n",
      "['bilaterian'] ['non-bilaterian'] 16\n",
      "['bilaterian'] ['non-bilaterian'] 16\n",
      "['e', 'start', 'value', 'of', 'a', 'routine', 'is', 'based', 'on', 'the', 'difficulty', 'of', 'the', 'elements', 'the', 'gymnast', 'attempts', 'and', 'whether', 'or', 'not', 'the', 'gymnast', 'meets', 'composition', 'requirements'] ['The', 'start', 'value', 'of', 'a', 'routine', 'is', 'based', 'on', 'the', 'difficulty', 'of', 'the', 'elements', 'the', 'gymnast', 'attempts', 'and', 'whether', 'or', 'not', 'the', 'gymnast', 'meets', 'composition', 'requirements'] 13\n",
      "['s', 'external', 'force', 'which', 'the', 'gymnasts', 'have', 'to', 'overcome', 'with', 'their', 'muscle', 'force', 'and', 'has', 'an', 'impact', 'on', 'the', 'gymnasts', 'linear', 'and', 'angular', 'momentum'] ['represents', 'external', 'force', 'which', 'the', 'gymnasts', 'have', 'to', 'overcome', 'with', 'their', 'muscle', 'force', 'and', 'has', 'an', 'impact', 'on', 'the', 'gymnasts', 'linear', 'and', 'angular', 'momentum'] 17\n",
      "['J.', 'Cruitt'] [] -1\n",
      "['Tom', 'Fleetwood'] ['game-Tom', 'Fleetwood'] 26\n",
      "['PictureTel', 'Corp', '.'] ['from', 'PictureTel', 'Corp.'] 115\n",
      "['U.S', '.'] ['The', 'U.S.'] 0\n",
      "['Exile', 'on', 'Main', 'St', '.'] ['with', 'Exile', 'on', 'Main', 'St.'] 12\n",
      "['vertebrate', 'diversity'] ['invertebrate', 'diversity'] 46\n",
      "['LF'] ['CR-LF'] 16\n",
      "['including', 'Martin', 'Luther', 'King', ',', 'Jr', '.'] [',', 'including', 'Martin', 'Luther', 'King', ',', 'Jr.'] 36\n",
      "['J.', 'Edgar', 'Hoover', 'Building'] [] -1\n",
      "['J.', 'Edgar', 'Hoover', 'Building', 'in', 'Washington', ',', 'D.C', '.', ','] ['J', '.', 'Edgar', 'Hoover', 'Building', 'in', 'Washington', ',', 'D.C.', ','] 6\n",
      "['G.', 'Stanley', 'Hall'] [] -1\n",
      "['\"', 'Archibald', ',', 'certainly', 'not', '!', '\"'] ['line', '\"', 'Archibald', ',', 'certainly', 'not', '!\")'] 156\n",
      "['J.P', '.', 'Morgan', 'and', 'the', 'Vanderbilt', 'family'] ['by', 'J.P.', 'Morgan', 'and', 'the', 'Vanderbilt', 'family'] 78\n",
      "['Prussia'] ['Austro-Prussian'] 23\n",
      "['\"', 'Kings', 'of', 'the', 'Sea', '\"'] ['(\"', 'Kings', 'of', 'the', 'Sea', '\")'] 31\n",
      "['235'] ['Uranium-235'] 0\n",
      "['Sphingomonas', 'sp', '.'] ['A', 'Sphingomonas', 'sp.'] 60\n",
      "['238'] ['uranium-238'] 63\n",
      "['18th', 'century'] ['mid-18th', 'century'] 17\n",
      "['J.S', '.', 'Szymanski'] [',', 'J.S.', 'Szymanski'] 30\n",
      "['x.', 'Men', 'did', 'not', 'show', 'any', 'sexual', 'arousal', 'to', 'non-human', 'visual', 'stimuli', ','] [] -1\n",
      "['.', '[', 'the', ']', 'measure', '[', 'of', \"'\", 'the', 'complex', 'components', 'of', 'sexual', 'orientation', 'as', 'differentiated', 'from', 'other', 'aspects', 'of', 'sexual', 'identity', 'at', 'one', 'point', 'in', 'time', \"']\"] ['...', '[', 'the', ']', 'measure', '[', 'of', \"'\", 'the', 'complex', 'components', 'of', 'sexual', 'orientation', 'as', 'differentiated', 'from', 'other', 'aspects', 'of', 'sexual', 'identity', 'at', 'one', 'point', 'in', 'time', \"']\"] 64\n",
      "[\"'\", 'Sexuality'] ['\"\\'', 'Sexuality'] 92\n",
      "['Apple', 'Inc', '.'] ['Apple', 'Inc.', \"'\"] 132\n",
      "['1990s'] ['mid-1990s'] 2\n",
      "['Louis', 'Jones', ',', 'Jr', '.'] ['and', 'Louis', 'Jones', ',', 'Jr.'] 140\n",
      "['William', 'J.', 'Brennan', ',', 'Jr', '.'] ['and', 'William', 'J.', 'Brennan', ',', 'Jr.'] 116\n",
      "['chip'] ['NES-on-a-chip'] 97\n",
      "['7', 'books'] ['27', 'books'] 9\n",
      "['g', 'allows', 'information', 'from', 'the', 'outside', 'world', 'to', 'be', 'sensed', 'in', 'the', 'form', 'of', 'chemical', 'and', 'physical', 'stimuli', '.'] ['Encoding', 'allows', 'information', 'from', 'the', 'outside', 'world', 'to', 'be', 'sensed', 'in', 'the', 'form', 'of', 'chemical', 'and', 'physical', 'stimuli', '.'] 18\n",
      "['L.', 'Schwabe', 'and', 'O.', 'Wolf'] [] -1\n",
      "['Eli', 'Lilly', 'and', 'Co', '.'] ['at', 'Eli', 'Lilly', 'and', 'Co.'] 129\n",
      "['S.E', '.', 'Massengill', 'Company'] ['by', 'S.E.', 'Massengill', 'Company'] 13\n",
      "['S.E', '.', 'Massengill', 'Company', 'of', 'Tennessee'] ['by', 'S.E.', 'Massengill', 'Company', 'of', 'Tennessee'] 13\n",
      "['E.D', '.', 'Searle', 'and', 'Co'] ['by', 'E.D.', 'Searle', 'and', 'Co.'] 105\n",
      "['5', '%'] ['85', '%'] 19\n",
      "['yptian', 'Se'] [] -1\n",
      "['m', 'and', 'E'] [] -1\n",
      "['J.', 'G.', 'Droysen'] [] -1\n",
      "['J.', 'G.', 'Droysen'] [] -1\n",
      "['tes', '(', '3'] [] -1\n",
      "['255', 'B.C', '.'] ['In', '255', 'B.C.'] 0\n",
      "['y', 'of'] ['hereditary', 'office'] 46\n",
      "['dom', '.', 'The', 'resulting', 'Indo-Scythian', 'kingdom', 'seems', 'to', 'have', 'gradually', 'pushed', 'the', 'remaining', 'Indo-Greek'] ['kingdom', '.', 'The', 'resulting', 'Indo-Scythian', 'kingdom', 'seems', 'to', 'have', 'gradually', 'pushed', 'the', 'remaining', 'Indo-Greek'] 72\n",
      "['Persian'] ['Greco-Persian'] 98\n",
      "['Greek'] ['non-Greeks'] 23\n",
      "['Greeks'] ['non-Greeks'] 23\n",
      "['Ammon'] ['Zeus-Ammon'] 206\n",
      "['Demeter'] ['Isis-Demeter'] 216\n",
      "['C.', 'Préaux'] [] -1\n",
      "['n', 'March', '2006', ',', 'the', 'foundation', 'announced', 'a', 'US', '$', '5', 'million', 'grant', 'for', 'the', 'International', 'Justice', 'Mission', '(', 'IJM', ')'] ['In', 'March', '2006', ',', 'the', 'foundation', 'announced', 'a', 'US', '$', '5', 'million', 'grant', 'for', 'the', 'International', 'Justice', 'Mission', '(', 'IJM', ')'] 0\n",
      "['R.', 'D.', 'Crawford'] [] -1\n",
      "['Vlaams'] ['West-Vlaams'] 51\n",
      "['R.E', '.', 'Appleman'] [',', 'R.E.', 'Appleman'] 26\n",
      "['U.S', '.'] ['the', 'U.S.'] 1\n",
      "['U.S', '.'] ['.', 'U.S.'] 80\n",
      "['\"', 'Greece', ',', 'although', 'captured', ',', 'took', 'its', 'wild', 'conqueror', 'captive', '\"'] ['(\"', 'Greece', ',', 'although', 'captured', ',', 'took', 'its', 'wild', 'conqueror', 'captive', '\")'] 57\n",
      "['$', '173', 'billion'] ['($', '173', 'billion'] 21\n",
      "['\"', 'W.T.F', '.', '\"'] ['episode', '\"', 'W.T.F.', '\"'] 142\n",
      "['William', 'Nelson', 'Goodwin', ',', 'Jr', '.'] ['by', 'William', 'Nelson', 'Goodwin', ',', 'Jr.'] 80\n",
      "['1963'] [] -1\n",
      "['A.', 'C.', 'Gilbert'] [] -1\n",
      "['2,648.6', '/', 'km²'] [] -1\n",
      "['William', 'F.', 'Buckley', ',', 'Jr', '.'] ['thinker', 'William', 'F.', 'Buckley', ',', 'Jr.'] 85\n",
      "['William', 'F.', 'Buckley', ',', 'Jr', '.'] ['thinker', 'William', 'F.', 'Buckley', ',', 'Jr.'] 85\n",
      "['John', 'DeStefano', ',', 'Jr', '.'] ['When', 'John', 'DeStefano', ',', 'Jr.'] 0\n",
      "['C.', 'Montgomery', 'Burns'] [] -1\n",
      "['orida', 'had', 'become', '\"', 'a', 'derelict', 'open', 'to', 'the', 'occupancy', 'of', 'every', 'enemy', ',', 'civilized', 'or', 'savage'] ['Florida', 'had', 'become', '\"', 'a', 'derelict', 'open', 'to', 'the', 'occupancy', 'of', 'every', 'enemy', ',', 'civilized', 'or', 'savage'] 73\n",
      "['966', 'Claude', 'R.', 'Kirk', ',', 'Jr.', 'was', 'elected', 'as', 'the', 'first', 'post-Reconstruction', 'Republican', 'governor', ',', 'in', 'an', 'upset', 'election'] ['1966', 'Claude', 'R.', 'Kirk', ',', 'Jr.', 'was', 'elected', 'as', 'the', 'first', 'post-Reconstruction', 'Republican', 'governor', ',', 'in', 'an', 'upset', 'election'] 84\n",
      "['968', 'Edward', 'J.', 'Gurney', ',', 'also', 'a', 'white', 'conservative', ',', 'was', 'elected', 'as', 'the', 'state', \"'s\", 'first', 'post-reconstruction', 'Republican', 'US', 'Senator'] ['1968', 'Edward', 'J.', 'Gurney', ',', 'also', 'a', 'white', 'conservative', ',', 'was', 'elected', 'as', 'the', 'state', \"'s\", 'first', 'post-reconstruction', 'Republican', 'US', 'Senator'] 105\n",
      "['4', '%'] ['24', '%'] 45\n",
      "['communism'] ['anti-communism'] 169\n",
      "['green'] ['blue-green'] 45\n",
      "['W.E', '.'] [',', 'W.E.'] 100\n",
      "['\"', 'Loop', '\"'] ['(\"', 'Loop', '\")'] 126\n",
      "['19th', 'century'] ['mid-19th', 'century'] 33\n",
      "['19th', 'century'] ['mid-19th', 'century'] 33\n",
      "['5', 'international', 'airlines', 'and', 'increasing', 'numbers', 'of', 'airlines', 'have', 'began', 'launching', 'direct', 'flights', 'from', 'Japan', ',', 'Qatar', ',', 'Taiwan', ',', 'South', 'Korea', ',', 'Germany', 'and', 'Singapore', '.'] ['15', 'international', 'airlines', 'and', 'increasing', 'numbers', 'of', 'airlines', 'have', 'began', 'launching', 'direct', 'flights', 'from', 'Japan', ',', 'Qatar', ',', 'Taiwan', ',', 'South', 'Korea', ',', 'Germany', 'and', 'Singapore', '.'] 33\n",
      "['M.', 'Jumel'] [] -1\n",
      "['U.S', '.'] ['by', 'U.S.'] 20\n",
      "['\"', 'Here', 'It', 'Comes', ',', 'Senors', '.', '..\"'] ['caption', '\"', 'Here', 'It', 'Comes', ',', 'Senors', '...\"'] 69\n",
      "['the', 'worst', 'record', 'of', 'labor', 'tension', 'of', 'any', 'university', 'in', 'the', 'U.S', '.'] ['having', 'the', 'worst', 'record', 'of', 'labor', 'tension', 'of', 'any', 'university', 'in', 'the', 'U.S.'] 38\n",
      "['L.', 'Bol', '.'] ['L', '.', 'Bol.'] 54\n",
      "['Corn', 'Law'] ['anti-Corn', 'Law'] 42\n",
      "['rotor'] ['self-rotors'] 37\n",
      "['Saharan', 'trade', 'routes', 'lost', 'significance', '.'] [] -1\n",
      "['president'] ['semi-presidential'] 66\n",
      "['J.', 'C.', 'Raulston', 'Arboretum'] [] -1\n",
      "['U.S', '.'] ['the', 'U.S.'] 19\n",
      "['1830'] ['1820-1830'] 1\n",
      "['ء', 'Mahmoud', 'Massahi'] [',ء', 'Mahmoud', 'Massahi'] 25\n",
      "['Its-Self'] [] -1\n",
      "['Mandarin'] ['non-Mandarin'] 24\n",
      "['5', '%'] ['85', '%'] 75\n",
      "['rain', 'forest'] ['tropical', 'rainforest'] 13\n",
      "['six'] ['thirty-six'] 4\n",
      "['J.', 'Robert', 'Oppenheimer', 'and', 'Edward', 'Teller'] [] -1\n",
      "['Soviet'] ['Nazi-Soviet'] 5\n",
      "['6'] ['26'] 3\n",
      "['H.', 'Becker'] [] -1\n",
      "['The', 'Edwin', 'Smith', 'papyrus'] ['Mediterranean.The', 'Edwin', 'Smith', 'papyrus'] 77\n",
      "['A.', 'Aaboe'] [] -1\n",
      "['Arabic'] ['Hindu-Arabic'] 77\n",
      "['1877'] ['1865-1877'] 12\n",
      "['Hobart', 'Taylor', 'Jr', '.'] [',', 'Hobart', 'Taylor', 'Jr.'] 49\n",
      "['Washington', 'D.C', '.'] ['in', 'Washington', 'D.C.'] 7\n",
      "['Level', '2', 'have', 'never', 'reached', 'the', 'final', '.'] ['Level', '2', 'have', 'never', 'reached', 'the', 'final.', '['] 69\n",
      "['C.', 'W.', 'Alcock'] [] -1\n",
      "['The', 'final', 'is', 'normally', 'held', 'the', 'Saturday', 'after', 'the', 'Premier', 'League', 'season', 'finishes', 'in', 'May', '.'] ['The', 'final', 'is', 'normally', 'held', 'the', 'Saturday', 'after', 'the', 'Premier', 'League', 'season', 'finishes', 'in', 'May.', 'The'] 0\n",
      "['e', 'semi-finals', 'have', 'been', 'played', 'exclusively', 'at', 'the', 'rebuilt', 'Wembley', 'Stadium'] ['The', 'semi-finals', 'have', 'been', 'played', 'exclusively', 'at', 'the', 'rebuilt', 'Wembley', 'Stadium'] 0\n",
      "['R.', 'J.', 'Reynolds', 'Tobacco', 'Company'] [] -1\n",
      "['e', 'wings', 'of', 'flightless', 'birds', 'and', 'the', 'rudiments', 'of', 'pelvis', 'and', 'leg', 'bones', 'found', 'in', 'some', 'snakes'] ['the', 'wings', 'of', 'flightless', 'birds', 'and', 'the', 'rudiments', 'of', 'pelvis', 'and', 'leg', 'bones', 'found', 'in', 'some', 'snakes'] 102\n",
      "['Communist'] ['anti-Communist'] 58\n",
      "['9', 'a.m', '.'] ['(', '9', 'a.m.'] 10\n",
      "['3', 'p.m', '.'] ['(', '3', 'p.m.'] 21\n",
      "['Lk', '.', '23:28-31'] ['Lk.', '23:28-31', ']'] 120\n",
      "['Washington', ',', 'D.C', '.'] ['after', 'Washington', ',', 'D.C.'] 73\n",
      "['2'] ['67.2'] 60\n",
      "['October'] ['mid-October'] 11\n",
      "['is', ',', 'to', 'the', 'devout', ',', 'taboo', '.'] ['is', ',', 'to', 'the', 'devout', ',', 'taboo.', '['] 149\n",
      "['P.', 'G.', 'A.', 'H.', 'Voigt'] [] -1\n",
      "['R.C', '.', 'Moyer'] ['by', 'R.C.', 'Moyer'] 11\n",
      "['any', 'electronic', 'dance', 'music', 'and', 'hip', 'hop', 'releases', 'today', 'are', 'still', 'preferred', 'on', 'vinyl'] ['Many', 'electronic', 'dance', 'music', 'and', 'hip', 'hop', 'releases', 'today', 'are', 'still', 'preferred', 'on', 'vinyl'] 0\n",
      "['May', '1986'] ['pre-May', '1986'] 80\n",
      "['Essentials', 'Jr', '.'] ['\"', 'Essentials', 'Jr.'] 12\n",
      "['on-demand'] ['made-on-demand'] 87\n",
      "['Socialist', 'parties'] ['elections.Socialist', 'parties'] 70\n",
      "['\"', 'Boy', ',', 'Up', '!\"', 'or', '\"', 'Boy', ',', 'Queue', '!', '\"'] [',', '\"', 'Boy', ',', 'Up', '!\"', 'or', '\"', 'Boy', ',', 'Queue', '!\"'] 48\n",
      "['Cork', 'Opera', 'House', '(', 'capacity', 'c.1000', ')', ',', 'Cyprus', 'Avenue', ',', 'Triskel', 'Christchurch', ',', 'the', 'Roundy', ',', 'the', 'Savoy', 'and', 'Coughlan', \"'s\"] ['the', 'Cork', 'Opera', 'House', '(', 'capacity', 'c.1000', ')', ',', 'Cyprus', 'Avenue', ',', 'Triskel', 'Christchurch', ',', 'the', 'Roundy', ',', 'the', 'Savoy', 'and', \"Coughlan's.\"] 95\n",
      "['ederalism', 'in', 'the', 'United', 'States', 'is', 'the', 'evolving', 'relationship', 'between', 'state', 'governments', 'and', 'the', 'federal', 'government', 'of', 'the', 'United', 'States'] ['Federalism', 'in', 'the', 'United', 'States', 'is', 'the', 'evolving', 'relationship', 'between', 'state', 'governments', 'and', 'the', 'federal', 'government', 'of', 'the', 'United', 'States'] 0\n",
      "['Movements', 'associated', 'with', 'the', 'establishment', 'or', 'development', 'of', 'federations', 'can', 'exhibit', 'either', 'centralising', 'or', 'decentralising', 'trends', '.'] [']', 'Movements', 'associated', 'with', 'the', 'establishment', 'or', 'development', 'of', 'federations', 'can', 'exhibit', 'either', 'centralising', 'or', 'decentralising', 'trends.'] 28\n",
      "['it', 'is', 'federal', ',', 'not', 'national', ';', 'in', 'the', 'sources', 'from', 'which', 'the', 'ordinary', 'powers', 'of', 'the', 'Government', 'are', 'drawn', ',', 'it', 'is', 'partly', 'federal', ',', 'and', 'partly', 'national.', '.'] [] -1\n",
      "['8', '%', 'of', 'public', 'spending', ',', '38', '%', 'for', 'the', 'regional', 'governments', ',', '13', '%', 'for', 'the', 'local', 'councils', ',', 'and', 'the', 'remaining', '31', '%', 'for', 'the', 'social', 'security', 'system'] ['18', '%', 'of', 'public', 'spending', ',', '38', '%', 'for', 'the', 'regional', 'governments', ',', '13', '%', 'for', 'the', 'local', 'councils', ',', 'and', 'the', 'remaining', '31', '%', 'for', 'the', 'social', 'security', 'system'] 51\n",
      "['6'] ['1,362'] 21\n",
      "['Berry', 'Gordy', ',', 'Jr', '.'] ['Berry', 'Gordy', ',', 'Jr.', 'founded'] 0\n",
      "['9,400,000', 'square', 'kilometres', '(', '3,600,000', 'sq', 'mi', ')'] [] -1\n",
      "['A.', 'V.', 'Dicey'] [] -1\n",
      "['c.', '3200', 'to', '1300', 'BCE'] [] -1\n",
      "['U.S', '.'] ['The', 'U.S.'] 0\n",
      "['i'] [] -1\n",
      "['three'] ['twenty-three'] 94\n",
      "['$', '2,644'] ['($', '2,644'] 190\n",
      "['acceptance', 'criteria', ',', 'fee', 'schedule', ',', 'etc', '.'] ['same', 'acceptance', 'criteria', ',', 'fee', 'schedule', ',', 'etc.'] 75\n",
      "['C.E', '.', 'Raven'] ['Professor', 'C.E.', 'Raven'] 69\n",
      "['I.M', '.', 'Pei'] ['by', 'I.M.', 'Pei'] 142\n",
      "['musette'] ['Bal-musette'] 0\n",
      "['Paris', 'Saint-Germain', 'F.C', '.'] ['club', 'Paris', 'Saint-Germain', 'F.C.'] 9\n",
      "['SCO', 'v', '.', 'IBM'] ['see', 'SCO', 'v.', 'IBM'] 116\n",
      "['Moscardi', ',', 'N.D', '.'] ['(', 'Moscardi', ',', 'N.D.'] 50\n",
      "['W.', 'Klement'] [] -1\n",
      "['~', '70', '%'] ['(~', '70', '%'] 78\n",
      "['9', '%'] ['19', '%'] 138\n",
      "['t', 'parks', ',', 'schools', ',', 'public', 'buildings', ',', 'proper', 'roads', 'and', 'the', 'other', 'amenities', 'that', 'characterise', 'a', 'modern', 'city'] ['without', 'parks', ',', 'schools', ',', 'public', 'buildings', ',', 'proper', 'roads', 'and', 'the', 'other', 'amenities', 'that', 'characterise', 'a', 'modern', 'city'] 95\n",
      "['early', 'morning', 'hours', '(', 'typically', 'from', '1:00', 'a.m.', 'to', '6:00', 'a.m', '.', ')'] ['the', 'early', 'morning', 'hours', '(', 'typically', 'from', '1:00', 'a.m.', 'to', '6:00', 'a.m.', ')'] 41\n",
      "['from', '5:00', 'p.m.', 'to', '6:30', 'p.m', '.'] ['generally', 'from', '5:00', 'p.m.', 'to', '6:30', 'p.m.'] 22\n",
      "['E', '=', 'mc²'] [] -1\n",
      "['2'] ['10,200'] 1\n",
      "['F.', 'A.', 'Hayek'] [] -1\n",
      "['s', 'when', 'too', 'much', 'water', 'is', 'drawn', 'into', 'the', 'bowels'] ['occurs', 'when', 'too', 'much', 'water', 'is', 'drawn', 'into', 'the', 'bowels'] 2\n",
      "['tuberculosis', ',', 'colon', 'cancer', ',', 'and', 'enteritis', '.'] ['tuberculosis', ',', 'colon', 'cancer', ',', 'and', 'enteritis.', '['] 86\n",
      "['23'] ['1,468,623'] 20\n",
      "['2:30', 'a.m', '.'] ['At', '2:30', 'a.m.'] 50\n",
      "['Kilvert', '&', 'Forbes', 'Ltd', '.'] [',', 'Kilvert', '&', 'Forbes', 'Ltd.'] 83\n",
      "['K.', 'Dun', 'Gifford'] [] -1\n",
      "['North', 'Carolina'] ['D-North', 'Carolina.'] 20\n",
      "['realists'] ['anti-realists'] 6\n",
      "['$', '1,000,000'] ['($', '1,000,000'] 194\n",
      "['-', 'belligerent', 'Hanover'] ['now', 'somehow-belligerent', 'Hanover'] 96\n",
      "[\"'\", 'Well', ',', 'it', 'still', 'might', 'be', 'true', '.', '\\'\"'] [',', \"'\", 'Well', ',', 'it', 'still', 'might', 'be', 'true.', '\\'\"'] 106\n",
      "['U.S', '.'] ['and', 'U.S.'] 34\n",
      "['U.K', '.'] ['The', 'U.K.'] 32\n",
      "['U.S', '.'] ['the', 'U.S.'] 132\n",
      "['Arabic'] ['non-Arabic'] 50\n",
      "['25,000-30', ','] ['25,000-30,000', 'Turkish'] 49\n",
      "['1975'] ['mid-1975'] 76\n",
      "['y', 'science-fiction', 'and', 'adventure'] ['early', 'science-fiction', 'and', 'adventure'] 21\n",
      "['J.', 'G.', 'Ballard'] [] -1\n",
      "['#', '9'] ['(#', '9'] 103\n",
      "['H.', 'G.', 'Wells'] [] -1\n",
      "['Washington', ',', 'D.C', '.'] ['in', 'Washington', ',', 'D.C.'] 59\n",
      "['8', '%'] ['18', '%'] 26\n",
      "['200'] ['8,200'] 14\n",
      "['six'] ['fifty-six'] 5\n",
      "['handheld', 'plungers', 'in', 'water', '.'] ['handheld', 'plungers', 'in', 'water.', '['] 36\n",
      "['Uthmanic'] ['pre-Uthmanic'] 94\n",
      "['A.', 'J.', 'P.', 'Taylor'] [] -1\n",
      "['central', 'portions', 'of', 'the', 'U.S', '.'] ['In', 'central', 'portions', 'of', 'the', 'U.S.'] 0\n",
      "['the', 'transition', 'of', 'American', 'manufacturing', 'to', 'the', 'industrial', 'revolution', '.'] ['the', 'transition', 'of', 'American', 'manufacturing', 'to', 'the', 'industrial', 'revolution.', '['] 80\n",
      "['gain'] [] -1\n",
      "['40', 'million', 'years', 'ago'] ['140', 'million', 'years', 'ago'] 70\n",
      "['1948.', 'On', '17', 'September'] [] -1\n",
      "['repeated', 'damage', 'to', 'their', 'tongues', ',', 'eyes', ',', 'joints', ',', 'skin', ',', 'and', 'muscles'] ['carelessly-repeated', 'damage', 'to', 'their', 'tongues', ',', 'eyes', ',', 'joints', ',', 'skin', ',', 'and', 'muscles'] 34\n",
      "['1980s'] ['mid-1980s'] 55\n",
      "[\"'\", 'A', \"'\", 'Mountain'] ['\"\\'', 'A', \"'\", 'Mountain'] 7\n",
      "['Hughes', 'Aircraft', 'Co', '.'] ['formerly', 'Hughes', 'Aircraft', 'Co.'] 45\n",
      "['Intuit', 'Inc', '.'] [',', 'Intuit', 'Inc.'] 55\n",
      "['Boulder', ',', 'Colo', '.'] ['while', 'Boulder', ',', 'Colo.'] 23\n",
      "['4'] ['FR-4'] 45\n",
      "['\"', 'Kill', 'all', ',', 'Burn', 'all', ',', 'Loot', 'all', '\"'] ['(\"', 'Kill', 'all', ',', 'Burn', 'all', ',', 'Loot', 'all', '\")'] 83\n",
      "['V.', 'Adm', '.', 'Takeo', 'Kurita'] [] -1\n",
      "['c.', '500', 'BC'] [] -1\n",
      "['c.', '488', '–', '444', 'Ma', 'and', 'early', 'Silurian', 'period'] [] -1\n",
      "['\"', 'to', 'lead', 'across', '\"'] ['(\"', 'to', 'lead', 'across', '\"'] 71\n",
      "['Super', 'Famicom', 'Jr', '.'] ['redesigned', 'Super', 'Famicom', 'Jr.'] 87\n",
      "['2', 'Mbit'] ['32', 'Mbit'] 94\n",
      "['\"', 'units', 'of', 'meaning', '\"'] ['(\"', 'units', 'of', 'meaning', '\")'] 61\n",
      "['How', 'can', 'there', 'be', 'absence', 'of', 'sin', 'where', 'there', 'is', 'concupiscence', '(', 'libido', ')', '?'] ['\"', 'How', 'can', 'there', 'be', 'absence', 'of', 'sin', 'where', 'there', 'is', 'concupiscence', '(', 'libido', ')?\"'] 149\n",
      "['the', 'language', 'used', 'in', 'Kievan', \"Rus'\"] [] -1\n",
      "['bomber'] ['fighter-bombers'] 15\n",
      "['Major', 'General', 'Charles', 'J.', 'Dunlap', ',', 'Jr', '.'] ['.', 'Major', 'General', 'Charles', 'J.', 'Dunlap', ',', 'Jr.'] 24\n",
      "['\"', 'One', 'From', 'On', 'High', '\"'] ['(\"', 'One', 'From', 'On', 'High', '\")'] 21\n",
      "['Nick', 'Holonyak', ',', 'Jr', '.'] ['by', 'Nick', 'Holonyak', ',', 'Jr.'] 11\n",
      "['>', '70', 'lm', '/', 'W'] ['(>', '70', 'lm', '/', 'W'] 120\n",
      "['Confucian'] ['neo-Confucian'] 28\n",
      "['L.', 'Junius', 'Brutus'] [] -1\n",
      "['L.', 'Junius', 'Brutus'] [] -1\n",
      "['divine', 'ancestor'] ['semi-divine', 'ancestor'] 38\n",
      "['Reynolds', 'v', '.', 'U.S', '.'] ['case', 'Reynolds', 'v', '.', 'U.S.'] 11\n",
      "['.', '38', '%'] ['to', '3.38', '%'] 66\n",
      "['6', '%'] ['22.6', '%'] 9\n",
      "['1828'] ['1826-1828'] 153\n",
      "['male'] ['all-male'] 43\n",
      "['D.D.E', '.'] ['\"', 'D.D.E.'] 76\n",
      "['Rubén', 'Díaz', ',', 'Jr', '.'] ['member', 'Rubén', 'Díaz', ',', 'Jr.'] 47\n",
      "['C.', 'B.', 'J.', 'Snyder'] [] -1\n",
      "['P.', 'Terentius', 'Afer'] [] -1\n",
      "['A.', 'C.', 'Crombie'] [] -1\n",
      "['20th', 'century'] ['mid-20th', 'century'] 12\n",
      "['because', 'of', 'the', 'miasma', 'theory', 'of', 'disease', ';', 'thus', ',', 'cemeteries', 'were', 'moved', 'out', 'of', 'town', ',', 'etc', '.'] ['important', 'because', 'of', 'the', 'miasma', 'theory', 'of', 'disease', ';', 'thus', ',', 'cemeteries', 'were', 'moved', 'out', 'of', 'town', ',', 'etc.'] 97\n",
      "['s', 'typically', 'discarded'] ['was', 'typically', 'discarded'] 43\n"
     ]
    }
   ],
   "source": [
    "print(\"Cases in which the start index was not detected correctly:\\n\")\n",
    "\n",
    "for incorrect_index in incorrect_lines_indices:\n",
    "    line = data[incorrect_index]\n",
    "    \n",
    "    original_answer_list, found_answer, index = extract_answer(line)\n",
    "\n",
    "    print(original_answer_list, found_answer, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9800636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87599it [01:37, 896.92it/s] \n"
     ]
    }
   ],
   "source": [
    "indicies_without_answer = copy.deepcopy(incorrect_lines_indices)\n",
    "\n",
    "for line_index, line in tqdm.tqdm(enumerate(data)):\n",
    "    updated_context, new_index, original_answer_list, tokens, index = insert_ne_tags(line)\n",
    "\n",
    "    if line_index not in incorrect_lines_indices:\n",
    "        new_last_index = find_last_index(updated_context, new_index, original_answer_list, tokens, index)\n",
    "        \n",
    "        if new_last_index == -1:\n",
    "            indicies_without_answer.append(line_index)\n",
    "\n",
    "        else:\n",
    "            data[line_index][\"answers\"][\"text\"] = [\" \".join(updated_context[new_index:new_last_index])]\n",
    "            \n",
    "            if new_index == 0:\n",
    "                data[line_index][\"answers\"][\"answer_start\"] = [0]\n",
    "            else:    \n",
    "                data[line_index][\"answers\"][\"answer_start\"] = [len(\" \".join(updated_context[:new_index])) + 1]\n",
    "\n",
    "    data[line_index][\"context\"] = \" \".join(updated_context)\n",
    "\n",
    "    if line_index in indicies_without_answer:\n",
    "        data[line_index][\"answers\"][\"text\"] = [\"\"]\n",
    "        data[line_index][\"answers\"][\"answer_start\"] = [-1]\n",
    "\n",
    "    columns_to_delete = [\"NER_context\", \"POS_context\", \"NER_question\", \"POS_question\"]    \n",
    "\n",
    "    for column_to_delete in columns_to_delete:\n",
    "        try:\n",
    "            del data[line_index][column_to_delete]\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Store the updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbf7d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(\"../../data/squad_data_train_ner_span.json\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
