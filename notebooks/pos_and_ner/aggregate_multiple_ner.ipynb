{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import groupby\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_file_path) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"../../data/squad_data_train_pos_ner.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Aggregate NE for spans for tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of all available tags:\n",
      "{'I-LOC', 'S-LOC', 'S-PER', 'I-ORG', 'B-ORG', 'E-ORG', 'B-MISC', 'S-MISC', 'I-MISC', 'E-PER', 'E-MISC', 'S-ORG', 'E-LOC', 'B-LOC', 'B-PER', 'I-PER'}\n",
      "Entities: {'MISC', 'LOC', 'ORG', 'PER'}\n"
     ]
    }
   ],
   "source": [
    "tags_set = set()\n",
    "\n",
    "for example in data:\n",
    "    for ners in example[\"NER_context\"]:\n",
    "        index = list(ners.keys())[0]\n",
    "        for ner in list(ners.values())[0]:\n",
    "            tags_set.add(ner[1])\n",
    "\n",
    "entities = set([tag.split(\"-\")[-1] for tag in list(tags_set)])\n",
    "print(f\"Set of all available tags:\\n{tags_set}\\nEntities: {entities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/87599 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [00:24<00:00, 3583.08it/s]\n"
     ]
    }
   ],
   "source": [
    "for line in tqdm(data):\n",
    "    # get indices of the tokens from the example\n",
    "    token_indices = [int(list(token.keys())[0]) for token in line[\"NER_context\"]]\n",
    "\n",
    "    # get spans of continuous indices\n",
    "    continuous_indices = [list(map(itemgetter(1), g)) for _, g in groupby(enumerate(token_indices), lambda i_x: i_x[0] - i_x[1])]\n",
    "\n",
    "    words_ne_each_span = []\n",
    "\n",
    "    # iterate over each indices span\n",
    "    for indices_span in continuous_indices:\n",
    "\n",
    "        # if the span is longer than 1\n",
    "        if len(indices_span) > 1:\n",
    "            \n",
    "            # get all the possible NE for all words within the span\n",
    "            words_ne_within_span = []\n",
    "\n",
    "            for index in indices_span:\n",
    "                for index_dict in line[\"NER_context\"]:\n",
    "                    if list(index_dict.keys())[0] == str(index):\n",
    "                        words_ne_within_span.append(list(index_dict.values())[0])\n",
    "\n",
    "            words_ne_each_span.append(words_ne_within_span)\n",
    "\n",
    "    # extract the ne tags for all words within each tag\n",
    "    ne_spans = []\n",
    "\n",
    "    for words_ne_span in words_ne_each_span:\n",
    "        ne_span = []\n",
    "\n",
    "        for word_ne in words_ne_span:\n",
    "            ne_span.append([value[1] for value in word_ne])\n",
    "\n",
    "        ne_spans.append(ne_span)\n",
    "\n",
    "    # aggregate NE tags at span level\n",
    "    # in this list we have the aggregated NE tags for the spans longer than 1\n",
    "    new_tags_spans = []\n",
    "\n",
    "    for ne_span in ne_spans:\n",
    "        new_tags_span = []\n",
    "        \n",
    "        found_agg_ne = False\n",
    "        max_occurence = 0\n",
    "        agg_entity = None\n",
    "\n",
    "        for entity in entities:\n",
    "            occurences_no = len([True for token in ne_span if entity in [tags.split(\"-\")[-1] for tags in token]])\n",
    "            \n",
    "            if occurences_no > max_occurence:\n",
    "                max_occurence = occurences_no\n",
    "                agg_entity = entity\n",
    "\n",
    "            if len(ne_span) == occurences_no:\n",
    "                new_tags_span.append(entity)\n",
    "                found_agg_ne = True\n",
    "\n",
    "        if found_agg_ne == False:\n",
    "            new_tags_span.append(agg_entity)\n",
    "\n",
    "        new_tags_spans.append(new_tags_span)\n",
    "\n",
    "    token_index_pos_map = {int(list(token.keys())[0]):index for index, token in enumerate(line[\"NER_context\"])}\n",
    "\n",
    "    longer_spans_index = -1\n",
    "\n",
    "    for indices_span in continuous_indices:\n",
    "        if len(indices_span) == 1:\n",
    "            tags = list(line[\"NER_context\"][token_index_pos_map[indices_span[0]]].values())[0]\n",
    "            new_tags = [[tag[0], tag[1].split(\"-\")[-1]] for tag in tags[:2]]\n",
    "            line[\"NER_context\"][token_index_pos_map[indices_span[0]]][str(indices_span[0])] = new_tags\n",
    "\n",
    "        else:\n",
    "            longer_spans_index += 1\n",
    "\n",
    "            for span_index in indices_span:\n",
    "                token = list(line[\"NER_context\"][token_index_pos_map[span_index]].values())[0][0][0]\n",
    "\n",
    "                new_tags = []\n",
    "\n",
    "                # if no token was found for the span, the most probable token is added\n",
    "                for tag in new_tags_spans[longer_spans_index]:\n",
    "                    new_tags.append([token, tag.split(\"-\")[-1]])\n",
    "\n",
    "                line[\"NER_context\"][token_index_pos_map[span_index]][str(span_index)] = new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(output_file_path, data):\n",
    "    # Open a new JSON file for writing\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        for data_line in data:\n",
    "            output_file.write(json.dumps(data_line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(\"../../data/squad_data_train_pos_ner_agg.json\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check aggregated NE for spans of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"../../data/squad_data_train_pos_ner.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg = read_data(\"../../data/squad_data_train_pos_ner_agg.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens that have more than 2 ne: 3010\n",
      "Number of tokens without ne: 0\n"
     ]
    }
   ],
   "source": [
    "more_than_two_ne = 0\n",
    "empty_ne = 0\n",
    "\n",
    "for line in data_agg:\n",
    "    for token in line[\"NER_context\"]:\n",
    "        ne_no = len(list(token.values())[0])\n",
    "\n",
    "        if ne_no > 2:\n",
    "            more_than_two_ne += 1\n",
    "        if ne_no == 0:\n",
    "            empty_ne += 1\n",
    "\n",
    "print(f\"Number of tokens that have more than 2 ne: {more_than_two_ne}\\nNumber of tokens without ne: {empty_ne}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for example with index 50000 there are spans that in the end do not have any NE tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': [['Beyoncé', 'S-PER', '0.97757655']]},\n",
       " {'19': [['US', 'S-LOC', '0.99831045']]},\n",
       " {'34': [['Destiny', 'B-ORG', '0.5214504'],\n",
       "   ['Destiny', 'B-MISC', '0.24072735']]},\n",
       " {'35': [[\"'s\", 'I-MISC', '0.5378261'], [\"'s\", 'I-ORG', '0.2589195']]},\n",
       " {'36': [['Child', 'E-MISC', '0.740782'], ['Child', 'E-ORG', '0.21769544']]},\n",
       " {'52': [['Recording', 'B-ORG', '0.7557777']]},\n",
       " {'53': [['Industry', 'I-ORG', '0.9865926']]},\n",
       " {'54': [['Association', 'I-ORG', '0.92794174']]},\n",
       " {'55': [['of', 'I-ORG', '0.99845254']]},\n",
       " {'56': [['America', 'E-ORG', '0.9987192']]},\n",
       " {'58': [['RIAA', 'S-ORG', '0.9977986']]},\n",
       " {'61': [['Beyoncé', 'S-ORG', '0.62609166'],\n",
       "   ['Beyoncé', 'S-PER', '0.26806518']]},\n",
       " {'81': [['Crazy', 'B-MISC', '0.98487943']]},\n",
       " {'82': [['in', 'I-MISC', '0.9824212']]},\n",
       " {'83': [['Love', 'E-MISC', '0.9944148']]},\n",
       " {'87': [['Single', 'B-MISC', '0.44712684']]},\n",
       " {'88': [['Ladies', 'E-MISC', '0.98573166']]},\n",
       " {'90': [['Put', 'B-MISC', '0.71322274']]},\n",
       " {'91': [['a', 'I-MISC', '0.99563205']]},\n",
       " {'92': [['Ring', 'I-MISC', '0.5038192'], ['Ring', 'E-MISC', '0.49564508']]},\n",
       " {'93': [['on', 'I-MISC', '0.6728376']]},\n",
       " {'94': [['It', 'E-MISC', '0.80424964'], ['It', 'I-MISC', '0.18712556']]},\n",
       " {'95': [[')\"', 'E-MISC', '0.25057456']]},\n",
       " {'98': [['Halo', 'S-MISC', '0.98769265']]},\n",
       " {'103': [['Irreplaceable', 'S-MISC', '0.9849631']]},\n",
       " {'119': [['The', 'B-ORG', '0.76986885'], ['The', 'B-MISC', '0.12658605']]},\n",
       " {'120': [['Observer', 'E-ORG', '0.8940206']]},\n",
       " {'127': [['Decade', 'E-MISC', '0.22428973']]},\n",
       " {'129': [['Billboard', 'S-MISC', '0.7208681'],\n",
       "   ['Billboard', 'E-MISC', '0.11272909']]},\n",
       " {'148': [['Billboard', 'S-MISC', '0.7524073'],\n",
       "   ['Billboard', 'S-PER', '0.1041657']]},\n",
       " {'164': [['Past', 'I-MISC', '0.17200282']]},\n",
       " {'165': [['25', 'I-MISC', '0.2823127']]},\n",
       " {'166': [['Years', 'I-MISC', '0.4467797'], ['Years', 'E-MISC', '0.3582096']]},\n",
       " {'175': [['VH1', 'S-ORG', '0.9058434']]},\n",
       " {'185': [['100', 'B-MISC', '0.12724186']]},\n",
       " {'186': [['Greatest', 'I-MISC', '0.13531509']]},\n",
       " {'187': [['Women', 'I-MISC', '0.12839265']]},\n",
       " {'189': [['Music', 'E-MISC', '0.30269274']]},\n",
       " {'192': [['Beyoncé', 'S-PER', '0.9896303']]},\n",
       " {'203': [['International', 'B-MISC', '0.7972138']]},\n",
       " {'204': [['Artist', 'I-MISC', '0.9515266']]},\n",
       " {'205': [['Award', 'E-MISC', '0.9984357']]},\n",
       " {'208': [['American', 'B-MISC', '0.9548852']]},\n",
       " {'209': [['Music', 'I-MISC', '0.9978619']]},\n",
       " {'210': [['Awards', 'E-MISC', '0.99975723']]},\n",
       " {'217': [['Legend', 'B-MISC', '0.9563908']]},\n",
       " {'218': [['Award', 'E-MISC', '0.9986951']]},\n",
       " {'222': [['World', 'I-MISC', '0.22286505'],\n",
       "   ['World', 'B-MISC', '0.12541336']]},\n",
       " {'223': [['Music', 'I-MISC', '0.6401172']]},\n",
       " {'224': [['Awards', 'E-MISC', '0.9984047']]},\n",
       " {'227': [['Billboard', 'B-MISC', '0.41367513']]},\n",
       " {'228': [['Millennium', 'I-MISC', '0.95103586']]},\n",
       " {'229': [['Award', 'E-MISC', '0.99954647']]},\n",
       " {'233': [['Billboard', 'I-MISC', '0.23133096'],\n",
       "   ['Billboard', 'B-MISC', '0.22580676']]},\n",
       " {'234': [['Music', 'I-MISC', '0.86562926']]},\n",
       " {'235': [['Awards', 'E-MISC', '0.99990904']]}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[888][\"NER_context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': [['Beyoncé', 'PER']]},\n",
       " {'19': [['US', 'LOC']]},\n",
       " {'34': [['Destiny', 'MISC'], ['Destiny', 'ORG']]},\n",
       " {'35': [[\"'s\", 'MISC'], [\"'s\", 'ORG']]},\n",
       " {'36': [['Child', 'MISC'], ['Child', 'ORG']]},\n",
       " {'52': [['Recording', 'ORG']]},\n",
       " {'53': [['Industry', 'ORG']]},\n",
       " {'54': [['Association', 'ORG']]},\n",
       " {'55': [['of', 'ORG']]},\n",
       " {'56': [['America', 'ORG']]},\n",
       " {'58': [['RIAA', 'ORG']]},\n",
       " {'61': [['Beyoncé', 'ORG'], ['Beyoncé', 'PER']]},\n",
       " {'81': [['Crazy', 'MISC']]},\n",
       " {'82': [['in', 'MISC']]},\n",
       " {'83': [['Love', 'MISC']]},\n",
       " {'87': [['Single', 'MISC']]},\n",
       " {'88': [['Ladies', 'MISC']]},\n",
       " {'90': [['Put', 'MISC']]},\n",
       " {'91': [['a', 'MISC']]},\n",
       " {'92': [['Ring', 'MISC']]},\n",
       " {'93': [['on', 'MISC']]},\n",
       " {'94': [['It', 'MISC']]},\n",
       " {'95': [[')\"', 'MISC']]},\n",
       " {'98': [['Halo', 'MISC']]},\n",
       " {'103': [['Irreplaceable', 'MISC']]},\n",
       " {'119': [['The', 'ORG']]},\n",
       " {'120': [['Observer', 'ORG']]},\n",
       " {'127': [['Decade', 'MISC']]},\n",
       " {'129': [['Billboard', 'MISC'], ['Billboard', 'MISC']]},\n",
       " {'148': [['Billboard', 'MISC'], ['Billboard', 'PER']]},\n",
       " {'164': [['Past', 'MISC']]},\n",
       " {'165': [['25', 'MISC']]},\n",
       " {'166': [['Years', 'MISC']]},\n",
       " {'175': [['VH1', 'ORG']]},\n",
       " {'185': [['100', 'MISC']]},\n",
       " {'186': [['Greatest', 'MISC']]},\n",
       " {'187': [['Women', 'MISC']]},\n",
       " {'189': [['Music', 'MISC']]},\n",
       " {'192': [['Beyoncé', 'PER']]},\n",
       " {'203': [['International', 'MISC']]},\n",
       " {'204': [['Artist', 'MISC']]},\n",
       " {'205': [['Award', 'MISC']]},\n",
       " {'208': [['American', 'MISC']]},\n",
       " {'209': [['Music', 'MISC']]},\n",
       " {'210': [['Awards', 'MISC']]},\n",
       " {'217': [['Legend', 'MISC']]},\n",
       " {'218': [['Award', 'MISC']]},\n",
       " {'222': [['World', 'MISC']]},\n",
       " {'223': [['Music', 'MISC']]},\n",
       " {'224': [['Awards', 'MISC']]},\n",
       " {'227': [['Billboard', 'MISC']]},\n",
       " {'228': [['Millennium', 'MISC']]},\n",
       " {'229': [['Award', 'MISC']]},\n",
       " {'233': [['Billboard', 'MISC']]},\n",
       " {'234': [['Music', 'MISC']]},\n",
       " {'235': [['Awards', 'MISC']]}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_agg[888][\"NER_context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check tags format when we have 18 classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
