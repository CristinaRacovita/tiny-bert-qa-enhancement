{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2416e8e8",
   "metadata": {},
   "source": [
    "### 0. Import libraries and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e8ca0b-2e9c-4884-8d26-a1cdf557a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import tqdm\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from flair.data import Sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6533324-85cf-4079-a104-d20a981f6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_file_path) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58fc9b90-51ea-463b-87d1-128d5ad44cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(output_file_path, data):\n",
    "    # Open a new JSON file for writing\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        for data_line in data:\n",
    "            output_file.write(json.dumps(data_line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae87b0ef-e953-4e00-ac30-890583a094dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"../../data/squad_data_validation_pos_ner_18_single_agg.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea924d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_TAGS = ['[DATE]', '[QUANTITY]', '[PRODUCT]', '[ORDINAL]', '[WORK_OF_ART]', '[LANGUAGE]', '[FAC]', '[EVENT]', '[ORG]', '[LAW]', '[CARDINAL]', '[GPE]', '[PERCENT]', '[NORP]', '[PERSON]', '[MONEY]', '[LOC]', '[TIME]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c484e",
   "metadata": {},
   "source": [
    "### 1. Find the token index where the answer starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4232297f-f49b-4f17-9fe9-5c04c6960153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subustring_in_string(string, substring, symbols):\n",
    "    for symbol in symbols:\n",
    "        if symbol in string and (substring in string or substring.replace(\" \", \"\") in string.replace(\" \", \"\")):\n",
    "           return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f685db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_one_text_in_context(text, context, symbols):\n",
    "    count = 0\n",
    "\n",
    "    for symbol in symbols:\n",
    "        if context.count(text + symbol):\n",
    "            count += 1\n",
    "\n",
    "    return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f02270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_start_position(text, pos_context, answer, context):\n",
    "    tokens = []\n",
    "    sentence = Sentence(text)\n",
    "    symbols = [\" \", \")\", \"(\", \"%\", \"-\", \".\", \",\"]\n",
    "\n",
    "    # get how many spaces are until the start position\n",
    "    spaces_no = context[:answer].count(\" \")\n",
    "\n",
    "    # get tokens of the answer\n",
    "    for word in sentence:\n",
    "        tokens.append(word.text)\n",
    "\n",
    "    start_position = 0\n",
    "    token_number = 0\n",
    "    match = False\n",
    "    no_match = -1\n",
    "\n",
    "    # check if the answer appear only once\n",
    "    one_apperance = is_one_text_in_context(text, context, symbols)\n",
    "\n",
    "    # iterate over positions in the pos context\n",
    "    for start_position in range(len(pos_context) - len(tokens) + 1):\n",
    "        # store in k_words_context_ and k_words_context the span of tokens from pos_context\n",
    "        k_words_context_ = [\n",
    "            bytes(list(line.values())[0][0], \"utf-8\").decode(\"unicode_escape\")\n",
    "            for line in pos_context[start_position : start_position + len(tokens)]\n",
    "        ]\n",
    "\n",
    "        k_words_context = [\n",
    "            list(line.values())[0][0].replace(\"\\\\\", \"\")\n",
    "            for line in pos_context[start_position : start_position + len(tokens)]\n",
    "        ]\n",
    "\n",
    "        # get the next word after the context\n",
    "        next_word = [\n",
    "            list(line.values())[0][0]\n",
    "            for line in pos_context[\n",
    "                start_position + len(tokens) : start_position + len(tokens) + 1\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "\n",
    "        # join all words in a string\n",
    "        k_words_context_string = \" \".join(k_words_context)\n",
    "        k_words_context_string_ = \" \".join(k_words_context_)\n",
    "\n",
    "        # define exact mach condition\n",
    "        exact_match = k_words_context == tokens or k_words_context_ == tokens\n",
    "\n",
    "\n",
    "        # if there is only one appearence and the answer was found, stop\n",
    "        if context.count(text) == 1 and exact_match:\n",
    "            match = True\n",
    "            break\n",
    "\n",
    "        # if there is only one appearence with the answer followed by a token, stop\n",
    "        if one_apperance and (\n",
    "            exact_match\n",
    "            or is_subustring_in_string(k_words_context_string, text, symbols)\n",
    "            or is_subustring_in_string(k_words_context_string_, text, symbols)\n",
    "        ):\n",
    "            match = True\n",
    "            break\n",
    "\n",
    "        # if the occurence is not the first one, but the start position is correct, stop\n",
    "        if (\n",
    "            text in k_words_context_string\n",
    "            or text in k_words_context_string_\n",
    "            or exact_match\n",
    "            or is_subustring_in_string(k_words_context_string, text, symbols)\n",
    "            or is_subustring_in_string(k_words_context_string_, text, symbols)\n",
    "        ) and (\n",
    "            token_number + spaces_no == answer or token_number + spaces_no + 1 == answer or token_number + spaces_no - 1 == answer\n",
    "        ):\n",
    "            match = True\n",
    "            break\n",
    "\n",
    "        # else consider the next word\n",
    "        elif next_word in symbols and (token_number + spaces_no == answer or token_number + spaces_no + 1 == answer or token_number + spaces_no - 1 == answer):\n",
    "            no_match = start_position\n",
    "            break\n",
    "\n",
    "        if \"'\" in k_words_context_:\n",
    "            token_number += len(k_words_context_[0])\n",
    "        else:\n",
    "            token_number += len(k_words_context[0])\n",
    "\n",
    "    if match:\n",
    "        return start_position\n",
    "    \n",
    "    return no_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(line, answer_index):\n",
    "    original_answer = line[\"answers\"][\"text\"][answer_index]\n",
    "    original_answer_start = line[\"answers\"][\"answer_start\"][answer_index]\n",
    "    original_answer_list = [token.text for token in list(Sentence(original_answer))]\n",
    "\n",
    "    index = find_start_position(original_answer, line[\"POS_context\"], original_answer_start, line[\"context\"])\n",
    "    found_answer = [list(token.values())[0][0] for token in line['POS_context'][index:index+len(original_answer_list)]]\n",
    "\n",
    "    return original_answer_list, found_answer, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc40ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_start_answer_detection(data):\n",
    "    incorrect_lines_indices = []\n",
    "    lines_without_any_answer = []\n",
    "    \n",
    "    for line_index, line in tqdm.tqdm(enumerate(data)):\n",
    "        has_answer = False\n",
    "\n",
    "        for answer_index in range(len(line[\"answers\"][\"text\"])):\n",
    "            original_answer_list, found_answer, index = extract_answer(line, answer_index)\n",
    "\n",
    "            if not \" \".join(found_answer).startswith(\" \".join(original_answer_list)):\n",
    "                incorrect_lines_indices.append(str(line_index) + \"_\" + str(answer_index))\n",
    "            else:\n",
    "                has_answer = True\n",
    "        \n",
    "        if not has_answer:\n",
    "            lines_without_any_answer.append(lines_without_any_answer)\n",
    "\n",
    "    print(f\"There are {len(lines_without_any_answer)} line without any answer.\")\n",
    "\n",
    "    return incorrect_lines_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f4233",
   "metadata": {},
   "source": [
    "### 2. Insert NER in the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96036c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_ne_tags(line, answer_index):\n",
    "    \"\"\"\n",
    "    Find start index for answer\n",
    "    \"\"\"\n",
    "    original_answer_list, found_answer, index = extract_answer(line, answer_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Get spans of continuous indices\n",
    "    \"\"\"\n",
    "    # get tokens\n",
    "    tokens = [list(token.values())[0][0] for token in line[\"POS_context\"]]\n",
    "\n",
    "    continuous_spans_indices = [\n",
    "        list(map(itemgetter(1), g))\n",
    "        for _, g in groupby(\n",
    "            enumerate([int(list(token.keys())[0]) for token in line[\"NER_context\"]]),\n",
    "            lambda i_x: i_x[0] - i_x[1],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    token_index_pos_map = {int(list(token.keys())[0]):index for index, token in enumerate(line[\"NER_context\"])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Insert NE tags and find the new start index for answer\n",
    "    \"\"\"\n",
    "    new_index = 0\n",
    "    updated_context = []\n",
    "    span_index = 0\n",
    "    token_index = 0\n",
    "    initial_token_index = 0\n",
    "\n",
    "    while token_index < len(tokens):\n",
    "        try:\n",
    "            continuous_spans_indices[span_index][0]\n",
    "        except:\n",
    "            updated_context.append(tokens[token_index])\n",
    "            if initial_token_index == index:\n",
    "                new_index = len(updated_context) - 1\n",
    "\n",
    "            token_index += 1\n",
    "            initial_token_index += 1\n",
    "            continue\n",
    "\n",
    "        if token_index + 1 > continuous_spans_indices[span_index][0] and len(continuous_spans_indices[span_index]) == 1:\n",
    "            ne_tags = [tag[1] for tag in list(line[\"NER_context\"][token_index_pos_map[token_index]].values())[0]]\n",
    "\n",
    "            for ne_tag in ne_tags:\n",
    "                updated_context.append(\"[\" + ne_tag + \"]\")\n",
    "\n",
    "            updated_context.append(tokens[token_index])\n",
    "\n",
    "            if initial_token_index == index:\n",
    "                \n",
    "                if updated_context[-2] in NER_TAGS:\n",
    "                    new_index = len(updated_context) - 1 - len(ne_tags)\n",
    "                else:\n",
    "                    new_index = len(updated_context) - 1\n",
    "\n",
    "            for ne_tag in ne_tags[::-1]:\n",
    "                updated_context.append(\"[\" + ne_tag + \"]\")\n",
    "\n",
    "            span_index += 1\n",
    "            token_index += 1\n",
    "            initial_token_index += 1\n",
    "\n",
    "\n",
    "        elif token_index + 1 > continuous_spans_indices[span_index][0] and len(continuous_spans_indices[span_index]) > 1:\n",
    "            ne_tags = [tag[1] for tag in list(line[\"NER_context\"][token_index_pos_map[token_index]].values())[0]]\n",
    "\n",
    "            for ne_tag in ne_tags:\n",
    "                updated_context.append(\"[\" + ne_tag + \"]\")\n",
    "\n",
    "            for i in range(len(continuous_spans_indices[span_index])):\n",
    "                updated_context.append(tokens[token_index])\n",
    "\n",
    "                if initial_token_index == index:\n",
    "                    if updated_context[-2] in NER_TAGS:\n",
    "                        new_index = len(updated_context) - 1 - len(ne_tags)\n",
    "                    else:\n",
    "                        new_index = len(updated_context) - 1\n",
    "\n",
    "                token_index += 1\n",
    "                initial_token_index += 1\n",
    "\n",
    "            for ne_tag in ne_tags[::-1]:\n",
    "                updated_context.append(\"[\" + ne_tag + \"]\")\n",
    "\n",
    "            span_index += 1\n",
    "\n",
    "        else:\n",
    "            updated_context.append(tokens[token_index])\n",
    "            if initial_token_index == index:\n",
    "                new_index = len(updated_context) - 1\n",
    "\n",
    "            token_index += 1\n",
    "            initial_token_index += 1\n",
    "\n",
    "        if token_index == len(tokens) - 1:\n",
    "            updated_context.append(tokens[token_index])\n",
    "            if initial_token_index == index:\n",
    "                new_index = len(updated_context) - 1\n",
    "\n",
    "    return updated_context, new_index, original_answer_list, tokens, index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4fb6e",
   "metadata": {},
   "source": [
    "### 3. Find last index of the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dbcd025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_index(updated_context, new_index, original_answer_list, tokens, index):\n",
    "    \"\"\"\n",
    "    Find last index of answer in the updated context\n",
    "    \"\"\"\n",
    "    new_last_index = new_index\n",
    "    searched_answer = copy.deepcopy(original_answer_list)\n",
    "\n",
    "    searching_answer_iter = -1\n",
    "\n",
    "    while len(searched_answer) != 0 or updated_context[new_last_index] in NER_TAGS:\n",
    "        searching_answer_iter += 1\n",
    "\n",
    "        if len(searched_answer) != 0:\n",
    "            if searched_answer[0] in updated_context[new_last_index]:\n",
    "                searched_answer.pop(0)\n",
    "                new_last_index += 1\n",
    "\n",
    "        try:\n",
    "            if updated_context[new_last_index] in NER_TAGS:\n",
    "                new_last_index += 1\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        if new_last_index == len(updated_context) or searching_answer_iter == 100:\n",
    "            break\n",
    "\n",
    "    original_answer = tokens[index:index+len(original_answer_list)]\n",
    "    updated_answer = updated_context[new_index:new_last_index]\n",
    "\n",
    "    if original_answer != [token for token in updated_answer if token not in NER_TAGS]:\n",
    "        return -1\n",
    "    \n",
    "    return new_last_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee80371",
   "metadata": {},
   "source": [
    "### 4. Process each line and update the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10570it [00:31, 340.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15 line without any answer.\n",
      "Number of answers for which the start index was not detected correctly: 167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "incorrect_lines_indices = check_start_answer_detection(data)\n",
    "print(f\"Number of answers for which the start index was not detected correctly: {len(incorrect_lines_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases in which the start index was not detected correctly:\n",
      "\n",
      "['8'] ['18'] 91\n",
      "['2'] ['12'] 65\n",
      "['10', '.'] ['2010', '.'] 54\n",
      "['7'] [] -1\n",
      "['39'] ['739'] 113\n",
      "['C.', 'J.', 'Anderson'] [] -1\n",
      "['C.', 'J.', 'Anderson'] [] -1\n",
      "['5'] ['1,135'] 67\n",
      "['C.', 'J.', 'Anderson'] [] -1\n",
      "['C.', 'J.', 'Anderson'] [] -1\n",
      "['L'] [] -1\n",
      "['L', '.'] ['Bowl', 'L.'] 52\n",
      "['0'] ['30'] 25\n",
      "['ten'] [] -1\n",
      "['two'] [] -1\n",
      "['28'] [] -1\n",
      "['Ted', 'Ginn', 'Jr', '.'] ['to', 'Ted', 'Ginn', 'Jr.'] 20\n",
      "['T.', 'J.', 'Ward'] [] -1\n",
      "['T.', 'J.', 'Ward'] [] -1\n",
      "['Ted', 'Ginn', 'Jr', '.'] ['to', 'Ted', 'Ginn', 'Jr.'] 20\n",
      "['Ted', 'Ginn', 'Jr', '.'] ['to', 'Ted', 'Ginn', 'Jr.'] 20\n",
      "['T.', 'J.', 'Ward', '.'] [] -1\n",
      "['T.', 'J.', 'Ward'] [] -1\n",
      "['Ted', 'Ginn', 'Jr', '.'] ['to', 'Ted', 'Ginn', 'Jr.'] 20\n",
      "['wards'] [] -1\n",
      "['wards'] [] -1\n",
      "['e', 'Red', 'Army'] ['the', 'Red', 'Army'] 4\n",
      "['Epte'] ['Saint-Clair-sur-Epte'] 53\n",
      "['American', 'humor', '.'] ['American', 'humor.', '\"'] 133\n",
      "['street', 'cars'] [\"'s\", 'streetcars'] 20\n",
      "['J.', 'P.', 'Morgan'] [] -1\n",
      "['J.', 'P.', 'Morgan'] [] -1\n",
      "['exactly', '8:10', 'p.m', '.', ','] ['from', 'exactly', '8:10', 'p.m.', ','] 15\n",
      "['her', 'weight', '.'] ['her', 'weight.', ':'] 23\n",
      "['t', 'in', 'the', 'student'] ['excitement', 'in', 'the', 'student'] 149\n",
      "['J.', 'S.', 'Bach'] [] -1\n",
      "['J.', 'S.', 'Bach'] [] -1\n",
      "['Ps', '.', '31:5'] ['(', 'Ps.', '31:5'] 49\n",
      "['Ps', '.', '31:5'] ['(', 'Ps.', '31:5'] 49\n",
      "['Ps', '.', '31:5'] ['(', 'Ps.', '31:5'] 49\n",
      "['1', 'a.m', '.'] ['At', '1', 'a.m.'] 61\n",
      "['2:45', 'a.m', '.'] ['at', '2:45', 'a.m.'] 14\n",
      "['60', \"'s\"] ['90-60', \"'s\"] 45\n",
      "['60', \"'s\"] ['90-60', \"'s\"] 45\n",
      "['60', \"'s\"] ['90-60', \"'s\"] 45\n",
      "['Riverside'] [] -1\n",
      "['Riverside'] [] -1\n",
      "['6,000', 'square', 'kilometres'] ['26,000', 'square', 'kilometres'] 2\n",
      "['6,000', 'square', 'kilometres'] ['26,000', 'square', 'kilometres'] 2\n",
      "['6,000', 'square', 'kilometres'] ['26,000', 'square', 'kilometres'] 2\n",
      "['562', 'to', '1598'] ['1562', 'to', '1598'] 78\n",
      "['Switzerland', 'and', 'the', 'Netherlands', '.'] ['Switzerland', 'and', 'the', 'Netherlands.', '['] 11\n",
      "['Southeastern', 'U.S', '.'] ['present-day', 'Southeastern', 'U.S.'] 28\n",
      "['UV'] [] -1\n",
      "['UV'] [] -1\n",
      "['sea', 'water'] ['whereas', 'seawater'] 109\n",
      "['magnet'] [] -1\n",
      "['air'] [] -1\n",
      "['nitrogen'] [] -1\n",
      "['U.S', '.'] ['the', 'U.S.'] 7\n",
      "['U.S', '.'] ['the', 'U.S.'] 7\n",
      "['the', 'U.S', '.'] ['that', 'the', 'U.S.'] 6\n",
      "['rst'] [] -1\n",
      "['ectrical', 'fire'] ['electrical', 'fire'] 60\n",
      "['Gemini'] ['all-Gemini'] 13\n",
      "['Gemini'] ['all-Gemini'] 13\n",
      "['Gemini'] ['all-Gemini'] 13\n",
      "['.', '2', 'billion', 'years', 'for', 'the', 'basaltic', 'samples', 'derived', 'from', 'the', 'lunar', 'maria', ',', 'to', 'about', '4.6', 'billion', 'years', 'for', 'samples', 'derived', 'from', 'the', 'highlands', 'crust'] ['about', '3.2', 'billion', 'years', 'for', 'the', 'basaltic', 'samples', 'derived', 'from', 'the', 'lunar', 'maria', ',', 'to', 'about', '4.6', 'billion', 'years', 'for', 'samples', 'derived', 'from', 'the', 'highlands', 'crust'] 28\n",
      "['\"', 'belt', 'animals', '\"'] ['(\"', 'belt', 'animals', '\")'] 2\n",
      "['Cambrian', 'period', '.'] ['mid-Cambrian', 'period', '.'] 98\n",
      "['M.', 'Theo', 'Kearney'] [] -1\n",
      "['M.', 'Theo', 'Kearney'] [] -1\n",
      "['M.', 'Theo', 'Kearney'] [] -1\n",
      "['y', 'through', 'the', 'port', \"'s\", 'trade', 'with', 'Constantinople', ',', 'and', 'ports', 'on', 'the', 'Black', 'Sea'] ['probably', 'through', 'the', 'port', \"'s\", 'trade', 'with', 'Constantinople', ',', 'and', 'ports', 'on', 'the', 'Black', 'Sea'] 55\n",
      "['J.I', '.', 'Pontanus'] ['J.I.', 'Pontanus', ':'] 33\n",
      "['J.I', '.', 'Pontanus'] ['J.I.', 'Pontanus', ':'] 33\n",
      "['J.I', '.', 'Pontanus'] ['J.I.', 'Pontanus', ':'] 33\n",
      "['J.', 'F.', 'D.', 'Shrewsbury'] [] -1\n",
      "['J.', 'F.', 'D.', 'Shrewsbury'] [] -1\n",
      "[\"chares'\"] [] -1\n",
      "['the', 'offices', 'and', 'board', 'room', 'etc', '.'] ['houses', 'the', 'offices', 'and', 'board', 'room', 'etc.'] 241\n",
      "['T.', 'T.', 'Tsui', 'Gallery', 'of', 'Chinese', 'art'] [] -1\n",
      "['8'] ['pre-1800'] 61\n",
      "['American', 'Broadcasting-Paramount', 'Theatres', ',', 'Inc', '.'] ['renamed', 'American', 'Broadcasting-Paramount', 'Theatres', ',', 'Inc.'] 92\n",
      "['Caris', '&', 'Co', '.'] ['by', 'Caris', '&', 'Co.'] 194\n",
      "['Caris', '&', 'Co', '.'] ['by', 'Caris', '&', 'Co.'] 194\n",
      "['Caris', '&', 'Co', '.'] ['by', 'Caris', '&', 'Co.'] 194\n",
      "['Agents', 'of', 'S.H.I.E.L.D', '.'] [',', 'Agents', 'of', 'S.H.I.E.L.D.'] 18\n",
      "['Sports', 'Programs', ',', 'Inc', '.'] ['company', 'Sports', 'Programs', ',', 'Inc.'] 57\n",
      "['Sports', 'Programs', ',', 'Inc', '.'] ['company', 'Sports', 'Programs', ',', 'Inc.'] 57\n",
      "['7:00', 'to', '9:00', 'a.m', '.'] ['from', '7:00', 'to', '9:00', 'a.m.'] 68\n",
      "['E.', 'W.', 'Scripps', 'Company'] [] -1\n",
      "['E.', 'W.', 'Scripps', 'Company'] [] -1\n",
      "['E.', 'W.', 'Scripps', 'Company'] [] -1\n",
      "['Warner', 'Bros', '.'] ['a', 'Warner', 'Bros.'] 64\n",
      "['Warner', 'Bros', '.'] ['a', 'Warner', 'Bros.'] 64\n",
      "['E.', 'W.', 'Scripps', 'Company'] [] -1\n",
      "['E.', 'W.', 'Scripps', 'Company'] [] -1\n",
      "['a', 'river', 'crevice', '.'] ['a', 'river', 'crevice.', '['] 87\n",
      "['storage', 'conditions', ',', 'compulsory', 'texts', ',', 'equipment', ',', 'etc', '.'] ['for', 'storage', 'conditions', ',', 'compulsory', 'texts', ',', 'equipment', ',', 'etc.'] 14\n",
      "['storage', 'conditions', ',', 'compulsory', 'texts', ',', 'equipment', ',', 'etc', '.'] ['for', 'storage', 'conditions', ',', 'compulsory', 'texts', ',', 'equipment', ',', 'etc.'] 14\n",
      "['the', 'checks', 'and', 'balances', 'system', 'of', 'the', 'U.S.', 'and', 'many', 'other', 'governments', '.'] ['the', 'checks', 'and', 'balances', 'system', 'of', 'the', 'U.S.', 'and', 'many', 'other', 'governments.', '['] 92\n",
      "['Doctor', 'of', 'Pharmacy', '(', 'Pharm.', 'D', '.', ')'] ['the', 'Doctor', 'of', 'Pharmacy', '(', 'Pharm.', 'D.', ')'] 178\n",
      "['>', '500', 'Da'] ['(>', '500', 'Da'] 2\n",
      "['>', '500', 'Da'] ['(>', '500', 'Da'] 2\n",
      "['>', '500', 'Da'] ['(>', '500', 'Da'] 2\n",
      "['violence'] ['non-violence'] 19\n",
      "['violence'] ['non-violence'] 19\n",
      "['t', 'to', 'render', 'certain', 'laws', 'ineffective', ',', 'to', 'cause', 'their', 'repeal', ',', 'or', 'to', 'exert', 'pressure', 'to', 'get', 'one', \"'s\", 'political', 'wishes', 'on', 'some', 'other', 'issue'] ['effort', 'to', 'render', 'certain', 'laws', 'ineffective', ',', 'to', 'cause', 'their', 'repeal', ',', 'or', 'to', 'exert', 'pressure', 'to', 'get', 'one', \"'s\", 'political', 'wishes', 'on', 'some', 'other', 'issue'] 29\n",
      "['tempted', 'to', 'enter', 'the', 'test', 'site'] ['attempted', 'to', 'enter', 'the', 'test', 'site'] 30\n",
      "['direct', 'civil', 'disobedience'] ['Indirect', 'civil', 'disobedience'] 11\n",
      "['direct', 'civil', 'disobedience'] ['Indirect', 'civil', 'disobedience'] 11\n",
      "['direct', 'civil', 'disobedience'] ['Indirect', 'civil', 'disobedience'] 11\n",
      "['direct', 'civil', 'disobedience'] ['Indirect', 'civil', 'disobedience'] 11\n",
      "['direct', 'civil', 'disobedience'] ['Indirect', 'civil', 'disobedience'] 11\n",
      "['W.', 'E.', 'B.', 'Du', 'Bois'] [] -1\n",
      "['W.', 'E.', 'B.', 'Du', 'Bois'] [] -1\n",
      "['W.', 'E.', 'B.', 'Du', 'Bois'] [] -1\n",
      "['over', '2', ','] ['over', '2,000', 'buildings'] 53\n",
      "['largest'] ['tenth-largest'] 43\n",
      "['largest'] ['tenth-largest'] 43\n",
      "['\"', 'push', '\"', 'motivations'] ['(\"', 'push', '\"', 'motivations'] 49\n",
      "['\"', 'push', '\"'] ['(\"', 'push', '\"'] 49\n",
      "['\"', 'pull', '\"'] ['(\"', 'pull', '\")'] 63\n",
      "['\"', 'pull', '\"'] ['(\"', 'pull', '\")'] 63\n",
      "['A.', 'A.', 'Michelson'] [] -1\n",
      "['A.', 'A.', 'Michelson'] [] -1\n",
      "['A.', 'A.', 'Michelson'] [] -1\n",
      "['授時暦'] ['(授時暦)'] 42\n",
      "['授時暦'] ['(授時暦)'] 42\n",
      "['.', '6-million-year-old'] ['a', '1.6-million-year-old'] 86\n",
      "['h', 'International', 'Criminal', 'Court', 'trial', 'dates', 'in', '2013', 'for', 'both', 'President', 'Kenyatta', 'and', 'Deputy', 'President', 'William', 'Ruto'] ['With', 'International', 'Criminal', 'Court', 'trial', 'dates', 'in', '2013', 'for', 'both', 'President', 'Kenyatta', 'and', 'Deputy', 'President', 'William', 'Ruto'] 0\n",
      "['ea', ',', 'coffee', ',', 'sisal', ',', 'pyrethrum', ',', 'corn', ',', 'and', 'wheat'] ['Tea', ',', 'coffee', ',', 'sisal', ',', 'pyrethrum', ',', 'corn', ',', 'and', 'wheat'] 0\n",
      "['r', '20', '%', 'to', '25', '%'] ['for', '20', '%', 'to', '25', '%'] 78\n",
      "['onomist'] [] -1\n",
      "['Capitol', 'Hill', ',', 'Washington', ',', 'D.C', '.'] ['on', 'Capitol', 'Hill', ',', 'Washington', ',', 'D.C.'] 60\n",
      "['Capitol', 'Hill', ',', 'Washington', ',', 'D.C', '.'] ['on', 'Capitol', 'Hill', ',', 'Washington', ',', 'D.C.'] 60\n",
      "['Capitol', 'Hill', ',', 'Washington', ',', 'D.C', '.'] ['on', 'Capitol', 'Hill', ',', 'Washington', ',', 'D.C.'] 60\n",
      "['e', 'a', 'third', 'group', 'of', 'pigments', 'found', 'in', 'cyanobacteria'] ['are', 'a', 'third', 'group', 'of', 'pigments', 'found', 'in', 'cyanobacteria'] 1\n",
      "['[', '256kn', '+', '1', ',', '256k', '(', 'n', '+', '1', ')', '−', '1', ']', '.'] ['[', '256kn', '+', '1', ',', '256k', '(', 'n', '+', '1', ')', '−', '1', '].[', 'citation'] 142\n",
      "['s', '=', '−', '2', ',', '−', '4', ',', '.', '..', ','] ['for', 's', '=', '−', '2', ',', '−', '4', ',', '...', ','] 12\n",
      "['−', '2', ',', '−', '4', ',', '.', '..', ','] ['=', '−', '2', ',', '−', '4', ',', '...', ','] 14\n",
      "['G.', 'H.', 'Hardy'] [] -1\n",
      "['G.', 'H.', 'Hardy'] [] -1\n",
      "['G.', 'H.', 'Hardy'] [] -1\n",
      "['G.', 'H.', 'Hardy'] [] -1\n",
      "['(', 'n', '−', '1', ')', '!'] ['if', '(', 'n', '−', '1', ')!'] 138\n",
      "['(', 'n', '−', '1', ')', '!'] ['if', '(', 'n', '−', '1', ')!'] 138\n",
      "['(', 'n', '−', '1', ')', '!'] ['if', '(', 'n', '−', '1', ')!'] 138\n",
      "['(', 'n', '−', '1', ')', '!'] ['if', '(', 'n', '−', '1', ')!'] 138\n",
      "['(', 'n', '−', '1', ')', '!'] ['if', '(', 'n', '−', '1', ')!'] 138\n",
      "['s', 'in', 'the', 'Swiss', 'canton', 'of', 'Graubünden', 'in', 'the', 'southeastern', 'Swiss', 'Alps', ','] ['begins', 'in', 'the', 'Swiss', 'canton', 'of', 'Graubünden', 'in', 'the', 'southeastern', 'Swiss', 'Alps', ','] 25\n",
      "['\"', 'Donkey', '\")'] ['(\"', 'Donkey', '\")'] 102\n",
      "['\"', 'lower', 'lake', '\"'] ['(\"', 'lower', 'lake', '\")'] 18\n",
      "['~', '11,700', 'years', 'ago'] ['(~', '11,700', 'years', 'ago'] 6\n",
      "['~', '11,700', 'years', 'ago'] ['(~', '11,700', 'years', 'ago'] 6\n",
      "['~', '8,000', 'years', 'ago'] ['(~', '8,000', 'years', 'ago'] 47\n",
      "['~', '8,000', 'years', 'ago'] ['(~', '8,000', 'years', 'ago'] 47\n",
      "['Political'] ['stories.Political'] 43\n",
      "['Political'] ['stories.Political'] 43\n",
      "['Political'] ['stories.Political'] 43\n",
      "['Political'] ['stories.Political'] 43\n",
      "['Political'] ['stories.Political'] 43\n",
      "['18th', 'century'] ['mid-18th', 'century'] 36\n",
      "['assimilation'] ['’assimilation'] 64\n",
      "['assimilation'] ['’assimilation'] 64\n",
      "['assimilation'] ['’assimilation'] 64\n"
     ]
    }
   ],
   "source": [
    "print(\"Cases in which the start index was not detected correctly:\\n\")\n",
    "\n",
    "for incorrect_index in incorrect_lines_indices:\n",
    "    line_index, answer_index = [int(val) for val in incorrect_index.split(\"_\")]\n",
    "\n",
    "    line = data[line_index]\n",
    "    original_answer_list, found_answer, index = extract_answer(line, answer_index)\n",
    "\n",
    "    print(original_answer_list, found_answer, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9800636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10570it [00:39, 269.09it/s]\n"
     ]
    }
   ],
   "source": [
    "indicies_without_answer = copy.deepcopy(incorrect_lines_indices)\n",
    "\n",
    "for line_index, line in tqdm.tqdm(enumerate(data)):\n",
    "    number_of_answers = len(data[line_index][\"answers\"][\"text\"])\n",
    "\n",
    "    for answer_index in range(number_of_answers):\n",
    "        updated_context, new_index, original_answer_list, tokens, index = insert_ne_tags(line, answer_index)\n",
    "\n",
    "        if str(line_index) + \"_\" + str(answer_index) not in indicies_without_answer:\n",
    "            new_last_index = find_last_index(updated_context, new_index, original_answer_list, tokens, index)\n",
    "\n",
    "            if new_last_index == -1:\n",
    "                indicies_without_answer.append(str(line_index) + \"_\" + str(answer_index))\n",
    "\n",
    "            else:\n",
    "                data[line_index][\"answers\"][\"text\"][answer_index] = \" \".join(updated_context[new_index:new_last_index])\n",
    "                \n",
    "                if new_index == 0:\n",
    "                    data[line_index][\"answers\"][\"answer_start\"][answer_index] = 0\n",
    "                else:    \n",
    "                    data[line_index][\"answers\"][\"answer_start\"][answer_index] = len(\" \".join(updated_context[:new_index])) + 1\n",
    "\n",
    "        if str(line_index) + \"_\" + str(answer_index) in indicies_without_answer:\n",
    "            data[line_index][\"answers\"][\"text\"][answer_index] = \"\"\n",
    "            data[line_index][\"answers\"][\"answer_start\"][answer_index] = -1           \n",
    "\n",
    "\n",
    "    data[line_index][\"context\"] = \" \".join(updated_context) \n",
    "\n",
    "    columns_to_delete = [\"NER_context\", \"POS_context\", \"NER_question\", \"POS_question\"]    \n",
    "\n",
    "    for column_to_delete in columns_to_delete:\n",
    "        try:\n",
    "            del data[line_index][column_to_delete]\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Store the updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbf7d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(\"../../data/squad_data_validation_ner_span_single_18.json\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
