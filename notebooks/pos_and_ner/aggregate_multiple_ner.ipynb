{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import groupby\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(input_file_path) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"../../data/squad_data_validation_pos_ner.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Aggregate NE for spans for tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of all available tags:\n",
      "{'I-MISC', 'E-MISC', 'B-LOC', 'I-ORG', 'E-ORG', 'S-ORG', 'E-LOC', 'B-PER', 'S-PER', 'S-MISC', 'E-PER', 'S-LOC', 'B-ORG', 'I-PER', 'I-LOC', 'B-MISC'}\n",
      "Entities: {'LOC', 'MISC', 'ORG', 'PER'}\n"
     ]
    }
   ],
   "source": [
    "tags_set = set()\n",
    "\n",
    "for example in data:\n",
    "    for ners in example[\"NER_context\"]:\n",
    "        index = list(ners.keys())[0]\n",
    "        for ner in list(ners.values())[0]:\n",
    "            tags_set.add(ner[1])\n",
    "\n",
    "entities = set([tag.split(\"-\")[-1] for tag in list(tags_set)])\n",
    "print(f\"Set of all available tags:\\n{tags_set}\\nEntities: {entities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [00:09<00:00, 1098.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for line in tqdm(data):\n",
    "    # get indices of the tokens from the example\n",
    "    token_indices = [int(list(token.keys())[0]) for token in line[\"NER_context\"]]\n",
    "\n",
    "    # get spans of continuous indices\n",
    "    continuous_indices = [list(map(itemgetter(1), g)) for _, g in groupby(enumerate(token_indices), lambda i_x: i_x[0] - i_x[1])]\n",
    "\n",
    "    words_ne_each_span = []\n",
    "\n",
    "    # iterate over each indices span\n",
    "    for indices_span in continuous_indices:\n",
    "\n",
    "        # if the span is longer than 1\n",
    "        if len(indices_span) > 1:\n",
    "            \n",
    "            # get all the possible NE for all words within the span\n",
    "            words_ne_within_span = []\n",
    "\n",
    "            for index in indices_span:\n",
    "                for index_dict in line[\"NER_context\"]:\n",
    "                    if list(index_dict.keys())[0] == str(index):\n",
    "                        words_ne_within_span.append(list(index_dict.values())[0])\n",
    "\n",
    "            words_ne_each_span.append(words_ne_within_span)\n",
    "\n",
    "    # extract the ne tags for all words within each tag\n",
    "    ne_spans = []\n",
    "\n",
    "    for words_ne_span in words_ne_each_span:\n",
    "        ne_span = []\n",
    "\n",
    "        for word_ne in words_ne_span:\n",
    "            ne_span.append([value[1] for value in word_ne])\n",
    "\n",
    "        ne_spans.append(ne_span)\n",
    "\n",
    "    # aggregate NE tags at span level\n",
    "    # in this list we have the aggregated NE tags for the spans longer than 1\n",
    "    new_tags_spans = []\n",
    "\n",
    "    for ne_span in ne_spans:\n",
    "        new_tags_span = []\n",
    "        \n",
    "        found_agg_ne = False\n",
    "        max_occurence = 0\n",
    "        agg_entity = None\n",
    "\n",
    "        for entity in entities:\n",
    "            occurences_no = len([True for token in ne_span if entity in [tags.split(\"-\")[-1] for tags in token]])\n",
    "            \n",
    "            if occurences_no > max_occurence:\n",
    "                max_occurence = occurences_no\n",
    "                agg_entity = entity\n",
    "\n",
    "            if len(ne_span) == occurences_no:\n",
    "                new_tags_span.append(entity)\n",
    "                found_agg_ne = True\n",
    "\n",
    "        if found_agg_ne == False:\n",
    "            new_tags_span.append(agg_entity)\n",
    "\n",
    "        new_tags_spans.append(new_tags_span)\n",
    "\n",
    "    token_index_pos_map = {int(list(token.keys())[0]):index for index, token in enumerate(line[\"NER_context\"])}\n",
    "\n",
    "    longer_spans_index = -1\n",
    "\n",
    "    for indices_span in continuous_indices:\n",
    "        if len(indices_span) == 1:\n",
    "            tags = list(line[\"NER_context\"][token_index_pos_map[indices_span[0]]].values())[0]\n",
    "            new_tags = [[tag[0], tag[1].split(\"-\")[-1]] for tag in tags[:2]]\n",
    "            line[\"NER_context\"][token_index_pos_map[indices_span[0]]][str(indices_span[0])] = new_tags\n",
    "\n",
    "        else:\n",
    "            longer_spans_index += 1\n",
    "\n",
    "            for span_index in indices_span:\n",
    "                token = list(line[\"NER_context\"][token_index_pos_map[span_index]].values())[0][0][0]\n",
    "\n",
    "                new_tags = []\n",
    "\n",
    "                # if no token was found for the span, the most probable token is added\n",
    "                for tag in new_tags_spans[longer_spans_index]:\n",
    "                    new_tags.append([token, tag.split(\"-\")[-1]])\n",
    "\n",
    "                line[\"NER_context\"][token_index_pos_map[span_index]][str(span_index)] = new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(output_file_path, data):\n",
    "    # Open a new JSON file for writing\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        for data_line in data:\n",
    "            output_file.write(json.dumps(data_line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(\"../../data/squad_data_validation_pos_ner_18_agg.json\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check aggregated NE for spans of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"../../data/squad_data_validation_pos_ner_18.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg = read_data(\"../../data/squad_data_validation_pos_ner_18_agg.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens that have more than 2 ne: 339\n"
     ]
    }
   ],
   "source": [
    "more_than_two_ne = 0\n",
    "\n",
    "for line in data_agg:\n",
    "    for token in line[\"NER_context\"]:\n",
    "        ne_no = len(list(token.values())[0])\n",
    "\n",
    "        if ne_no > 2:\n",
    "            more_than_two_ne += 1\n",
    "\n",
    "print(f\"Number of tokens that have more than 2 ne: {more_than_two_ne}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'3': [['syrenka', 'S-PERSON', '0.37249884']]},\n",
       " {'6': [['Warsaw', 'S-GPE', '0.9950407']]},\n",
       " {'34': [['at', 'B-DATE', '0.11415275']]},\n",
       " {'35': [['least', 'I-DATE', '0.54797506']]},\n",
       " {'36': [['the', 'I-DATE', '0.5037803']]},\n",
       " {'37': [['mid-14th', 'I-DATE', '0.93928146']]},\n",
       " {'38': [['century', 'E-DATE', '0.9985323']]},\n",
       " {'46': [['Warsaw', 'S-GPE', '0.9953707']]},\n",
       " {'50': [['year', 'E-DATE', '0.28886825'],\n",
       "   ['year', 'I-DATE', '0.19738819'],\n",
       "   ['year', 'S-DATE', '0.10291117']]},\n",
       " {'51': [['1390', 'E-DATE', '0.98943406']]},\n",
       " {'61': [['Latin', 'S-NORP', '0.95007807']]},\n",
       " {'63': [['Sigilium', 'S-NORP', '0.28365117']]},\n",
       " {'64': [['Civitatis', 'E-PERSON', '0.106351845']]},\n",
       " {'65': [['Varsoviensis', 'E-ORG', '0.1934269'],\n",
       "   ['Varsoviensis', 'E-PERSON', '0.10567914']]},\n",
       " {'72': [['Warsaw', 'S-GPE', '0.9180408']]},\n",
       " {'75': [['City', 'S-GPE', '0.10257403']]},\n",
       " {'81': [['1609', 'S-DATE', '0.96907514']]},\n",
       " {'107': [['1653', 'S-DATE', '0.9954269']]},\n",
       " {'110': [['Zygmunt', 'B-PERSON', '0.9383151']]},\n",
       " {'111': [['Laukowski', 'E-PERSON', '0.99991286']]}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[888][\"NER_context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'3': [['syrenka', 'PERSON']]},\n",
       " {'6': [['Warsaw', 'GPE']]},\n",
       " {'34': [['at', 'DATE']]},\n",
       " {'35': [['least', 'DATE']]},\n",
       " {'36': [['the', 'DATE']]},\n",
       " {'37': [['mid-14th', 'DATE']]},\n",
       " {'38': [['century', 'DATE']]},\n",
       " {'46': [['Warsaw', 'GPE']]},\n",
       " {'50': [['year', 'DATE']]},\n",
       " {'51': [['1390', 'DATE']]},\n",
       " {'61': [['Latin', 'NORP']]},\n",
       " {'63': [['Sigilium', 'PERSON']]},\n",
       " {'64': [['Civitatis', 'PERSON']]},\n",
       " {'65': [['Varsoviensis', 'PERSON']]},\n",
       " {'72': [['Warsaw', 'GPE']]},\n",
       " {'75': [['City', 'GPE']]},\n",
       " {'81': [['1609', 'DATE']]},\n",
       " {'107': [['1653', 'DATE']]},\n",
       " {'110': [['Zygmunt', 'PERSON']]},\n",
       " {'111': [['Laukowski', 'PERSON']]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_agg[888][\"NER_context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check tags format when we have 18 classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
